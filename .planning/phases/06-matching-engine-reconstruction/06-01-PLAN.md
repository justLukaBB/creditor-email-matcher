---
phase: 06-matching-engine-reconstruction
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - app/models/matching_config.py
  - app/models/creditor_inquiry.py
  - app/models/match_result.py
  - app/models/__init__.py
  - alembic/versions/xxx_add_matching_infrastructure.py
autonomous: true

must_haves:
  truths:
    - "MatchingThreshold table exists with category-based configuration"
    - "CreditorInquiry model available in active codebase"
    - "MatchResult model ready for explainability JSONB storage"
  artifacts:
    - path: "app/models/matching_config.py"
      provides: "MatchingThreshold SQLAlchemy model"
      contains: "class MatchingThreshold"
    - path: "app/models/creditor_inquiry.py"
      provides: "CreditorInquiry SQLAlchemy model"
      contains: "class CreditorInquiry"
    - path: "app/models/match_result.py"
      provides: "MatchResult SQLAlchemy model with JSONB scoring_details"
      contains: "scoring_details = Column(JSONB"
  key_links:
    - from: "app/models/__init__.py"
      to: "app/models/matching_config.py"
      via: "import and __all__ export"
      pattern: "from app.models.matching_config import"
---

<objective>
Create database infrastructure for the matching engine: threshold configuration table, CreditorInquiry model, and MatchResult model with explainability JSONB.

Purpose: Database schema must exist before matching logic can query thresholds or store results.
Output: Three SQLAlchemy models + Alembic migration ready for deployment.
</objective>

<execution_context>
@/Users/luka.s/.claude/get-shit-done/workflows/execute-plan.md
@/Users/luka.s/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/06-matching-engine-reconstruction/06-CONTEXT.md
@.planning/phases/06-matching-engine-reconstruction/06-RESEARCH.md

# Existing models reference
@_existing-code/app/models/creditor_inquiry.py
@_existing-code/app/models/match_result.py
@app/models/__init__.py
@app/database.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create MatchingThreshold model for database-driven configuration</name>
  <files>app/models/matching_config.py</files>
  <action>
Create `app/models/matching_config.py` with MatchingThreshold SQLAlchemy model:

```python
class MatchingThreshold(Base):
    __tablename__ = "matching_thresholds"

    id = Column(Integer, primary_key=True)
    category = Column(String(50), nullable=False)  # "default", "bank", "inkasso"
    threshold_type = Column(String(50), nullable=False)  # "min_match", "gap_threshold"
    threshold_value = Column(Numeric(5, 4), nullable=False)  # 0.0000 to 1.0000
    weight_name = Column(String(50), nullable=True)  # "client_name", "reference"
    weight_value = Column(Numeric(5, 4), nullable=True)
    description = Column(Text, nullable=True)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())
```

Add unique constraint: `(category, threshold_type, weight_name)`
Add index: `idx_matching_thresholds_lookup` on `(category, threshold_type)`

CONTEXT.MD: Thresholds stored in PostgreSQL for runtime changes without deployment.
  </action>
  <verify>python -c "from app.models.matching_config import MatchingThreshold; print('OK')"</verify>
  <done>MatchingThreshold model imports without error and has all required columns</done>
</task>

<task type="auto">
  <name>Task 2: Add CreditorInquiry and MatchResult models to active codebase</name>
  <files>app/models/creditor_inquiry.py, app/models/match_result.py, app/models/__init__.py</files>
  <action>
**CreditorInquiry model** (`app/models/creditor_inquiry.py`):
- Copy from `_existing-code/app/models/creditor_inquiry.py`
- Keep exact schema (Node.js portal writes this table)
- Important columns: client_name, client_name_normalized, creditor_email, reference_number, sent_at

**MatchResult model** (`app/models/match_result.py`):
- Copy from `_existing-code/app/models/match_result.py`
- UPDATE scoring_details to use JSONB (not JSON): `scoring_details = Column(JSONB, nullable=True)`
- This enables structured queries on explainability data
- Add `ambiguity_gap` column: `Column(Numeric(5, 4), nullable=True)` for threshold calibration

**Update `app/models/__init__.py`**:
- Add imports for MatchingThreshold, CreditorInquiry, MatchResult
- Add to __all__ list

Note: Keep component score columns (client_name_score etc.) for backward compatibility even though we prefer JSONB.
  </action>
  <verify>python -c "from app.models import CreditorInquiry, MatchResult, MatchingThreshold; print('OK')"</verify>
  <done>All three models import from app.models package</done>
</task>

<task type="auto">
  <name>Task 3: Create Alembic migration with default threshold values</name>
  <files>alembic/versions/xxx_add_matching_infrastructure.py</files>
  <action>
Create migration using: `alembic revision -m "add_matching_infrastructure"`

Migration should:
1. Create `matching_thresholds` table with all columns and constraints
2. Create `match_results` table if not exists (with JSONB scoring_details)
3. Verify `creditor_inquiries` table exists (created by Node.js portal, should already exist)
4. Insert default configuration:

```sql
-- Default thresholds (CONTEXT.MD: 40% name, 60% reference)
INSERT INTO matching_thresholds (category, threshold_type, threshold_value, description) VALUES
  ('default', 'min_match', 0.7000, 'Minimum score for any match consideration'),
  ('default', 'gap_threshold', 0.1500, 'Gap between #1 and #2 for auto-match');

INSERT INTO matching_thresholds (category, weight_name, weight_value, description) VALUES
  ('default', 'client_name', 0.4000, 'Weight for client name signal'),
  ('default', 'reference_number', 0.6000, 'Weight for reference number signal');
```

5. Create indexes:
   - `idx_matching_thresholds_lookup` on `(category, threshold_type)`
   - `idx_match_results_email_id` on `incoming_email_id`
   - `idx_match_results_selected` on `(selected_as_match, calculated_at)` where selected_as_match=true

Use `op.execute()` for INSERT statements. Include downgrade to drop tables.
  </action>
  <verify>alembic upgrade head (or alembic check if DB not available)</verify>
  <done>Migration file exists with upgrade/downgrade, includes default thresholds</done>
</task>

</tasks>

<verification>
1. All three models import without error: `python -c "from app.models import MatchingThreshold, CreditorInquiry, MatchResult"`
2. Migration file exists in alembic/versions/
3. Migration includes default threshold configuration
4. JSONB type used for scoring_details (not JSON)
</verification>

<success_criteria>
- MatchingThreshold model with category-based configuration
- CreditorInquiry model copied from _existing-code (no schema changes)
- MatchResult model with JSONB scoring_details and ambiguity_gap column
- Alembic migration ready to run
- Default thresholds: min_match=0.70, gap_threshold=0.15, weights 40/60
</success_criteria>

<output>
After completion, create `.planning/phases/06-matching-engine-reconstruction/06-01-SUMMARY.md`
</output>
