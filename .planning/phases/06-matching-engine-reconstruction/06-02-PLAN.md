---
phase: 06-matching-engine-reconstruction
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - app/services/matching/__init__.py
  - app/services/matching/signals.py
  - app/services/matching/explainability.py
autonomous: true

must_haves:
  truths:
    - "Name matching uses RapidFuzz token_sort_ratio with explicit preprocessing"
    - "Reference matching handles OCR errors with fuzzy partial_ratio"
    - "Explainability builder produces JSONB-ready dict with signal scores"
  artifacts:
    - path: "app/services/matching/signals.py"
      provides: "Signal scorer functions for name and reference matching"
      exports: ["score_client_name", "score_reference_numbers"]
    - path: "app/services/matching/explainability.py"
      provides: "ExplainabilityBuilder for JSONB match explanations"
      exports: ["ExplainabilityBuilder"]
  key_links:
    - from: "app/services/matching/signals.py"
      to: "rapidfuzz"
      via: "import fuzz, utils"
      pattern: "from rapidfuzz import fuzz, utils"
---

<objective>
Implement signal scorers (name, reference) using RapidFuzz and explainability builder for JSONB match explanations.

Purpose: Signal scorers are the core matching logic; explainability enables debugging and threshold tuning.
Output: Reusable signal scoring functions and ExplainabilityBuilder class.
</objective>

<execution_context>
@/Users/luka.s/.claude/get-shit-done/workflows/execute-plan.md
@/Users/luka.s/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/06-matching-engine-reconstruction/06-CONTEXT.md
@.planning/phases/06-matching-engine-reconstruction/06-RESEARCH.md

# Existing v1 matching engine for reference
@app/services/matching_engine.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create signal scorers with RapidFuzz 3.x preprocessing</name>
  <files>app/services/matching/__init__.py, app/services/matching/signals.py</files>
  <action>
Create `app/services/matching/` package directory with `__init__.py`.

Create `app/services/matching/signals.py` with two signal scorer functions:

**1. score_client_name(inquiry_name, inquiry_name_normalized, extracted_name) -> tuple[float, dict]**
- Returns (score 0.0-1.0, scoring_details dict)
- Use RapidFuzz with EXPLICIT preprocessing (3.x requirement):
  ```python
  from rapidfuzz import fuzz, utils

  # Use processor parameter for automatic lowercase + punctuation removal
  token_sort = fuzz.token_sort_ratio(
      compare_name, extracted_name,
      processor=utils.default_process,
      score_cutoff=50  # Early exit optimization
  ) / 100
  ```
- Try multiple algorithms: token_sort_ratio, partial_ratio, token_set_ratio
- Return max score (best algorithm for this pair)
- Scoring details include: algorithm_used, inquiry_value, extracted_value, all_scores

RESEARCH PITFALL: RapidFuzz 3.x removed automatic preprocessing. Must use `processor=utils.default_process`.

**2. score_reference_numbers(inquiry_reference, extracted_references) -> tuple[float, dict]**
- Returns (score 0.0-1.0, scoring_details dict)
- Handle OCR errors with FUZZY matching (not just exact):
  - Exact match after normalization -> 1.0
  - Partial match (handles missing prefix/suffix) -> use partial_ratio with score_cutoff=80
  - Token-based (handles word order) -> use token_sort_ratio with score_cutoff=80
- Normalize: uppercase, strip whitespace
- Return best score across all extracted references
- Scoring details include: matched_reference, algorithm_used, raw_score

CONTEXT.MD: Reference number matching should handle OCR errors (fuzzy, not just exact).

**Package __init__.py exports:**
```python
from app.services.matching.signals import score_client_name, score_reference_numbers
__all__ = ["score_client_name", "score_reference_numbers"]
```
  </action>
  <verify>python -c "from app.services.matching import score_client_name, score_reference_numbers; print('OK')"</verify>
  <done>Both signal scorer functions import and are callable</done>
</task>

<task type="auto">
  <name>Task 2: Create ExplainabilityBuilder for JSONB match explanations</name>
  <files>app/services/matching/explainability.py, app/services/matching/__init__.py</files>
  <action>
Create `app/services/matching/explainability.py` with ExplainabilityBuilder class:

```python
class ExplainabilityBuilder:
    """
    Build JSONB explainability payloads for match results.

    CONTEXT.MD:
    - Primary audience: developers (for debugging and threshold tuning)
    - Detail level: signal scores only (no verbose reasoning)
    - Storage: PostgreSQL JSONB column on match_results
    - Retention: 90-day pruning (handled separately)
    """

    VERSION = "v2.0"  # Track explainability schema version

    @staticmethod
    def build(
        inquiry: "CreditorInquiry",
        extracted_data: dict,
        component_scores: dict[str, float],
        signal_details: dict[str, dict],
        final_score: float,
        match_status: str,
        gap: float | None = None,
        gap_threshold: float = 0.15,
        weights: dict[str, float] | None = None,
    ) -> dict:
        """
        Build explainability JSONB payload.

        Returns dict suitable for MatchResult.scoring_details JSONB column.
        """
        return {
            "version": ExplainabilityBuilder.VERSION,
            "match_status": match_status,  # auto_matched, ambiguous, below_threshold, no_candidates
            "final_score": round(final_score, 4),
            "gap": round(gap, 4) if gap is not None else None,
            "gap_threshold": gap_threshold,
            "signals": {
                "client_name": {
                    "score": round(component_scores.get("client_name", 0), 4),
                    "weighted_score": round(component_scores.get("client_name", 0) * weights.get("client_name", 0.4), 4),
                    "inquiry_value": inquiry.client_name,
                    "extracted_value": extracted_data.get("client_name"),
                    **signal_details.get("client_name", {})
                },
                "reference_number": {
                    "score": round(component_scores.get("reference", 0), 4),
                    "weighted_score": round(component_scores.get("reference", 0) * weights.get("reference_number", 0.6), 4),
                    "inquiry_value": inquiry.reference_number,
                    "extracted_value": extracted_data.get("reference_numbers"),
                    **signal_details.get("reference", {})
                }
            },
            "weights": weights or {"client_name": 0.40, "reference_number": 0.60},
            "filters_applied": {
                "creditor_inquiries_window_days": 30,
                "both_signals_required": True  # CONTEXT.MD decision
            },
            "inquiry_id": inquiry.id,
            "inquiry_sent_at": inquiry.sent_at.isoformat() if inquiry.sent_at else None
        }
```

Update `app/services/matching/__init__.py` to export ExplainabilityBuilder.
  </action>
  <verify>python -c "from app.services.matching import ExplainabilityBuilder; print('OK')"</verify>
  <done>ExplainabilityBuilder imports and build() method is callable</done>
</task>

</tasks>

<verification>
1. Signal scorers import: `from app.services.matching import score_client_name, score_reference_numbers`
2. ExplainabilityBuilder imports: `from app.services.matching import ExplainabilityBuilder`
3. Test signal scorer:
   ```python
   score, details = score_client_name("Mustermann, Max", "mustermann max", "Max Mustermann")
   assert 0.9 <= score <= 1.0  # Should match well
   ```
4. Test reference scorer with OCR error:
   ```python
   score, details = score_reference_numbers("AZ-12345", ["AZ-I2345"])  # 1->I OCR error
   assert score >= 0.8  # Should still match with fuzzy
   ```
</verification>

<success_criteria>
- score_client_name uses RapidFuzz with processor=utils.default_process
- score_reference_numbers handles OCR errors with fuzzy matching
- Both functions return (score, details) tuple
- ExplainabilityBuilder.build() returns JSONB-ready dict with version, signals, weights
- All exports from app/services/matching package
</success_criteria>

<output>
After completion, create `.planning/phases/06-matching-engine-reconstruction/06-02-SUMMARY.md`
</output>
