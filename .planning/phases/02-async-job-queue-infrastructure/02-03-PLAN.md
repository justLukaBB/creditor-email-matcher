---
phase: 02-async-job-queue-infrastructure
plan: 03
type: execute
wave: 2
depends_on: ["02-01", "02-02"]
files_modified:
  - app/actors/email_processor.py
  - app/routers/webhook.py
  - app/actors/__init__.py
autonomous: true

must_haves:
  truths:
    - "Webhook validates synchronously, saves email with RECEIVED status, queues Dramatiq job, returns 200 OK"
    - "Worker picks up job, transitions QUEUED -> PROCESSING -> COMPLETED/FAILED with FOR UPDATE SKIP LOCKED"
    - "Actor retries transient failures with exponential backoff (max 5 retries, 15s-5min)"
    - "Worker memory usage remains stable under continuous processing"
    - "Email notification fires only on permanent failure, after all retries exhausted"
  artifacts:
    - path: "app/actors/email_processor.py"
      provides: "Dramatiq actor for async email processing"
      exports: ["process_email"]
      contains: "@dramatiq.actor"
    - path: "app/routers/webhook.py"
      provides: "Refactored webhook that enqueues instead of BackgroundTasks"
      contains: "process_email.send"
  key_links:
    - from: "app/routers/webhook.py"
      to: "app/actors/email_processor.py"
      via: "process_email.send(email_id=id) enqueues Dramatiq message"
      pattern: "process_email\\.send"
    - from: "app/actors/email_processor.py"
      to: "app/models/incoming_email.py"
      via: "FOR UPDATE SKIP LOCKED query prevents duplicate processing"
      pattern: "with_for_update.*skip_locked"
    - from: "app/actors/email_processor.py"
      to: "app/services/dual_write.py"
      via: "Uses DualDatabaseWriter for creditor debt updates"
      pattern: "DualDatabaseWriter"
    - from: "app/actors/__init__.py"
      to: "app/actors/email_processor.py"
      via: "Imports actor to register with broker"
      pattern: "from app\\.actors import email_processor"
    - from: "app/actors/email_processor.py"
      to: "app/services/failure_notifier.py"
      via: "on_failure callback invokes notify_permanent_failure when max retries exhausted"
      pattern: "notify_permanent_failure"
---

<objective>
Create the email processing Dramatiq actor and refactor the webhook to enqueue jobs instead of using BackgroundTasks.

Purpose: This is the core of Phase 2. The webhook becomes a thin validation+enqueue layer, and the heavy processing moves to a Dramatiq actor with retry logic, state machine tracking, and memory management.
Output: Working email processor actor and refactored webhook handler.
</objective>

<execution_context>
@/Users/luka.s/.claude/get-shit-done/workflows/execute-plan.md
@/Users/luka.s/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/STATE.md
@.planning/phases/02-async-job-queue-infrastructure/02-RESEARCH.md
@.planning/phases/02-async-job-queue-infrastructure/02-CONTEXT.md
@.planning/phases/02-async-job-queue-infrastructure/02-01-SUMMARY.md
@.planning/phases/02-async-job-queue-infrastructure/02-02-SUMMARY.md
@app/routers/webhook.py
@app/services/dual_write.py
@app/services/idempotency.py
@app/database.py
@app/models/incoming_email.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create email processor Dramatiq actor</name>
  <files>app/actors/email_processor.py, app/actors/__init__.py</files>
  <action>
Create `app/actors/email_processor.py` with the process_email actor:

1. **Imports (top of file):**
   - `import gc` -- explicit garbage collection after each job
   - `import dramatiq`
   - `import structlog`
   - `import psutil`
   - `from datetime import datetime`
   - Other imports lazy (inside function body) to avoid circular deps

2. **Retry predicate function** `should_retry(retries_so_far, exception)`:
   - Retry on: RateLimitError (from anthropic), ConnectionError, TimeoutError, OperationalError (from sqlalchemy)
   - Do NOT retry on: BadRequestError (from anthropic), ValueError, KeyError -- these are permanent failures
   - Return `retries_so_far < 5` for retryable exceptions, False for permanent

3. **On-failure callback** `on_process_email_failure(actor, message, exception)`:
   - This is called by Dramatiq when the actor has permanently failed (all retries exhausted OR non-retryable exception).
   - Extract email_id from message.args or message.kwargs
   - Lazy import: `from app.services.failure_notifier import notify_permanent_failure`
   - Call `notify_permanent_failure(email_id)` -- this sends the email alert
   - Wrap in try/except to never crash the worker (notification is best-effort)
   - Log the permanent failure with structlog: `logger.error("permanent_job_failure", email_id=email_id, error=str(exception))`

   IMPORTANT (CONTEXT.md decision): "Email notification on permanent failure (after all retries exhausted)". This callback is the ONLY place failure notifications are sent. Do NOT send notifications in the actor's exception handler -- that would fire on every transient retry, violating the user's decision.

4. **Actor definition** `@dramatiq.actor(max_retries=5, min_backoff=15000, max_backoff=300000, retry_when=should_retry, on_failure=on_process_email_failure, queue_name="email_processing")`:

5. **process_email(email_id: int)** function body:
   - Lazy import SessionLocal from app.database (avoid circular deps)
   - Create new database session: `db = SessionLocal()`
   - Use try/except/finally pattern with db.close() in finally
   - Log memory before processing using psutil

   **Processing steps inside try block:**
   a. Load and lock email row: `db.query(IncomingEmail).filter(IncomingEmail.id == email_id).with_for_update(skip_locked=True).first()`
   b. If None (locked by another worker or not found), log warning and return
   c. Check if already completed: `if email.processing_status in ("completed", "failed"): return`
   d. Transition to "processing": set `email.processing_status = "processing"`, `email.started_at = datetime.utcnow()`, `db.commit()`
   e. Parse email: use `email_parser.parse_email(html_body=email.raw_body_html, text_body=email.raw_body_text)`
   f. Store parsed data: `email.cleaned_body = parsed["cleaned_body"]`, token counts, status = "parsed", `db.commit()`
   g. Extract entities: choose extractor based on `settings.llm_provider`, call `extractor.extract_entities(...)`
   h. Store extracted data: `email.extracted_data = extracted_entities.model_dump()`, status = "extracted", `db.commit()`
   i. Check is_creditor_reply -- if False, set status "not_creditor_reply", match_status "no_match", commit and return
   j. Match and write: Use DualDatabaseWriter saga pattern (same logic as current webhook.py process_incoming_email)
   k. Set status "completed", `email.completed_at = datetime.utcnow()`, `email.processed_at = datetime.utcnow()`, `db.commit()`
   l. Send email notification on auto-match (same as current webhook logic)

   **Exception handling (except block):**
   - Load email fresh, set processing_status = "failed", processing_error = str(e), increment retry_count, commit
   - Re-raise the exception so Dramatiq retry logic (and eventually on_failure callback) kicks in
   - Do NOT send failure notification here -- Dramatiq's on_failure callback handles that ONLY after all retries are exhausted

   **Finally block:**
   - `db.close()`
   - `gc.collect()` -- explicit garbage collection for memory stability under 512MB constraint
   - Log memory after gc using psutil

IMPORTANT: The imports for email_parser, entity_extractor, entity_extractor_claude, mongodb_service, email_notifier, DualDatabaseWriter, IdempotencyService, generate_idempotency_key should all be lazy (inside the function) to avoid import-time side effects in worker processes.

6. **Update app/actors/__init__.py**: Uncomment/add the actor import line:
   ```python
   from app.actors import email_processor  # noqa: F401
   ```
   This registers the actor with the broker when the actors package is imported.
  </action>
  <verify>
Run `python -c "from app.actors.email_processor import process_email; print(process_email.actor_name)"` -- should print "process_email".
Run `python -c "from app.actors.email_processor import should_retry; print(should_retry(0, TimeoutError()))"` -- should print True.
Run `python -c "from app.actors.email_processor import should_retry; print(should_retry(0, ValueError()))"` -- should print False.
Run `python -c "from app.actors.email_processor import on_process_email_failure; print(callable(on_process_email_failure))"` -- should print True.
Verify gc import: `grep -q 'import gc' app/actors/email_processor.py && echo 'OK' || echo 'FAIL: missing gc import'`.
Verify NO notification in except block: `grep -A5 'except.*Exception' app/actors/email_processor.py | grep -q 'notify\|notification\|failure_notifier' && echo 'FAIL: notification in except block' || echo 'OK: no notification in except block'`.
  </verify>
  <done>
Email processor actor exists at app/actors/email_processor.py with: Dramatiq actor decorator (max_retries=5, exponential backoff 15s-5min, on_failure=on_process_email_failure), selective retry predicate, FOR UPDATE SKIP LOCKED row locking, full state machine transitions, DualDatabaseWriter integration, gc.collect() memory management, gc module imported. On-failure callback invokes notify_permanent_failure ONLY after all retries exhausted. No failure notification in actor exception handler. Actor registered in app/actors/__init__.py.
  </done>
</task>

<task type="auto">
  <name>Task 2: Refactor webhook to validate-and-enqueue pattern</name>
  <files>app/routers/webhook.py</files>
  <action>
Refactor `app/routers/webhook.py` to the validate -> save -> enqueue -> return 200 pattern:

1. **Remove** the `process_incoming_email()` async function entirely (this logic now lives in the actor).

2. **Remove** the `BackgroundTasks` import and parameter from the endpoint.

3. **Remove** imports that are only used by processing logic: email_parser, entity_extractor, entity_extractor_claude, MatchingEngine, zendesk_client, email_notifier, DualDatabaseWriter, IdempotencyService, generate_idempotency_key. The actor handles these now.

4. **Keep** imports for: IncomingEmail, get_db, settings, webhook_schemas, hmac/hashlib for signature verification.

5. **Refactor** the `receive_webhook()` endpoint to this flow:
   a. Verify webhook signature (same as now)
   b. Deduplication check (same as now)
   c. Create IncomingEmail with processing_status="received" -- include `attachment_urls=webhook_data.attachments` (new field from Plan 02)
   d. Commit to PostgreSQL (audit trail guaranteed)
   e. Transition to "queued": `incoming_email.processing_status = "queued"`, commit
   f. Enqueue Dramatiq job: `from app.actors.email_processor import process_email; process_email.send(email_id=incoming_email.id)`
   g. Return `{"status": "accepted", "message": "Email queued for processing", "email_id": incoming_email.id}`

6. **Handle MongoDB-only mode**: If `db is None`, log a warning that async processing requires PostgreSQL. Return 503 with message "Async processing requires PostgreSQL. Configure DATABASE_URL." -- MongoDB-only mode without PostgreSQL cannot use Dramatiq (no state machine).

7. **Keep** the existing `get_email_status()` endpoint as-is (it still queries IncomingEmail).

8. **Use structlog** instead of stdlib logging (consistent with rest of codebase). Replace `import logging` / `logger = logging.getLogger(__name__)` with `import structlog` / `logger = structlog.get_logger()`.

IMPORTANT decisions from CONTEXT.md:
- Simple 200 OK response -- no job ID returned to Zendesk
- Webhook saves incoming_email to PostgreSQL (RECEIVED) synchronously, THEN queues
- Zendesk gets 200 only if validation passes
  </action>
  <verify>
Run `python -c "from app.routers.webhook import router; print([r.path for r in router.routes])"` -- should show /webhook and /status/{email_id}.
Verify process_incoming_email function is gone: `grep -q 'process_incoming_email' app/routers/webhook.py && echo 'FAIL: old function still present' || echo 'OK: old function removed'`.
Verify enqueue call exists: `grep -q 'process_email.send' app/routers/webhook.py && echo 'OK' || echo 'FAIL'`.
Verify BackgroundTasks removed: `grep -q 'BackgroundTasks' app/routers/webhook.py && echo 'FAIL' || echo 'OK'`.
  </verify>
  <done>
Webhook endpoint validates synchronously, saves to PostgreSQL with RECEIVED status (including attachment_urls), transitions to QUEUED, enqueues Dramatiq job via process_email.send(), returns 200 OK. Old process_incoming_email background task removed. MongoDB-only mode returns 503.
  </done>
</task>

</tasks>

<verification>
- Actor registered with broker: `python -c "from app.actors import email_processor"`
- Webhook enqueues instead of BackgroundTasks: no BackgroundTasks in webhook.py
- State machine transitions in actor: grep for "processing_status" shows received/queued/processing/completed/failed
- FOR UPDATE SKIP LOCKED in actor: prevents duplicate processing
- gc.collect() in actor: memory management for 512MB constraint (gc module imported)
- on_failure callback wired: `grep -q 'on_failure' app/actors/email_processor.py`
- Failure notification only on permanent failure: no notify calls in except block
</verification>

<success_criteria>
- Webhook is a thin validation+enqueue layer (< 100 lines)
- Actor contains all processing logic with retry, state machine, and memory management
- No BackgroundTasks used anywhere
- Dramatiq job created with process_email.send(email_id=N)
- Failure notification fires ONLY via on_failure callback (after all retries exhausted), never on transient retries
</success_criteria>

<output>
After completion, create `.planning/phases/02-async-job-queue-infrastructure/02-03-SUMMARY.md`
</output>
