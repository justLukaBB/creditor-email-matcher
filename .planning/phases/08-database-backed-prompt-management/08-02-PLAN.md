---
phase: 08-database-backed-prompt-management
plan: 02
type: execute
wave: 2
depends_on: ["08-01"]
files_modified:
  - app/services/prompt_renderer.py
  - app/services/prompt_manager.py
  - app/services/prompt_metrics_service.py
autonomous: true

must_haves:
  truths:
    - "PromptRenderer can render Jinja2 templates with variable interpolation"
    - "PromptVersionManager handles activation/rollback of ANY historical version"
    - "get_active_prompt returns the currently active prompt for task_type+name"
    - "PromptMetricsService records extraction-level metrics linked to prompt version"
  artifacts:
    - path: "app/services/prompt_renderer.py"
      provides: "Jinja2 template rendering with validation"
      contains: "class PromptRenderer"
    - path: "app/services/prompt_manager.py"
      provides: "Version activation, rollback, get_active_prompt"
      contains: "class PromptVersionManager"
    - path: "app/services/prompt_metrics_service.py"
      provides: "Extraction-level metrics recording"
      contains: "def record_extraction_metrics"
  key_links:
    - from: "app/services/prompt_manager.py"
      to: "app/models/prompt_template.py"
      via: "SQLAlchemy queries"
      pattern: "PromptTemplate.*is_active"
    - from: "app/services/prompt_metrics_service.py"
      to: "app/models/prompt_metrics.py"
      via: "SQLAlchemy insert"
      pattern: "PromptPerformanceMetrics"
---

<objective>
Create prompt management services: PromptRenderer for Jinja2 templating, PromptVersionManager for activation/rollback, and PromptMetricsService for recording extraction-level performance metrics.

Purpose: Enable runtime prompt loading, rendering, and performance tracking without code changes.
Output: Service layer for prompt management ready for extractor integration.
</objective>

<execution_context>
@/Users/luka.s/.claude/get-shit-done/workflows/execute-plan.md
@/Users/luka.s/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/08-database-backed-prompt-management/08-CONTEXT.md
@.planning/phases/08-database-backed-prompt-management/08-RESEARCH.md
@.planning/phases/08-database-backed-prompt-management/08-01-SUMMARY.md
@app/models/prompt_template.py
@app/models/prompt_metrics.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create PromptRenderer with Jinja2 templating</name>
  <files>app/services/prompt_renderer.py</files>
  <action>
Create PromptRenderer service per RESEARCH.md Pattern 2:

```python
from jinja2 import Environment, TemplateSyntaxError, UndefinedError
import structlog

logger = structlog.get_logger(__name__)

class PromptRenderer:
    """
    Renders Jinja2 prompt templates with variable validation.

    Handles:
    - Template syntax errors (log and raise)
    - Missing variables (log and raise)
    - Consistent Jinja2 environment configuration
    """

    def __init__(self):
        self.env = Environment(
            autoescape=False,  # LLM prompts don't need HTML escaping
            trim_blocks=True,
            lstrip_blocks=True,
        )

    def render(
        self,
        template_str: str,
        variables: dict,
        template_name: str = "unknown"
    ) -> str:
        """
        Render Jinja2 template with variables.

        Args:
            template_str: Jinja2 template string from database
            variables: Dict of variables to interpolate
            template_name: For error logging

        Returns:
            Rendered prompt string

        Raises:
            TemplateSyntaxError: Invalid Jinja2 syntax
            UndefinedError: Missing required variable
        """
        # Implementation per RESEARCH.md Pattern 2

    def validate_template(self, template_str: str) -> tuple[bool, str | None]:
        """
        Validate Jinja2 template syntax without rendering.

        Returns:
            (is_valid, error_message or None)

        Per RESEARCH.md Pitfall 5: validate syntax on creation.
        """
```

Include structlog logging for rendered template length, variables used, and any errors.
  </action>
  <verify>python -c "from app.services.prompt_renderer import PromptRenderer; r = PromptRenderer(); print(r.render('Hello {{ name }}', {'name': 'World'}))"</verify>
  <done>PromptRenderer can render templates with variable interpolation, validates syntax</done>
</task>

<task type="auto">
  <name>Task 2: Create PromptVersionManager for activation/rollback</name>
  <files>app/services/prompt_manager.py</files>
  <action>
Create PromptVersionManager per RESEARCH.md Pattern 4:

```python
from sqlalchemy.orm import Session
from sqlalchemy import and_, func
import structlog

from app.models.prompt_template import PromptTemplate

logger = structlog.get_logger(__name__)

class PromptVersionManager:
    """
    Manages prompt version lifecycle: create, activate, rollback.

    USER DECISIONS honored:
    - Explicit activation required (no auto-activation of latest)
    - Rollback to ANY historical version (not just previous)
    - Free-form names
    """

    def __init__(self, db: Session):
        self.db = db

    def get_active_prompt(self, task_type: str, name: str) -> PromptTemplate | None:
        """
        Get currently active prompt template.

        Returns:
            Active PromptTemplate or None if no active version
        """

    def activate_version(
        self,
        task_type: str,
        name: str,
        version: int,
        activated_by: str
    ) -> PromptTemplate:
        """
        Activate a specific prompt version (atomically deactivates current).

        Per USER DECISION: explicit activation required.
        """

    def rollback_to_version(
        self,
        task_type: str,
        name: str,
        target_version: int,
        rolled_back_by: str
    ) -> PromptTemplate:
        """
        Rollback to ANY historical version.

        Per USER DECISION: not just previous version.
        """

    def create_new_version(
        self,
        task_type: str,
        name: str,
        user_prompt_template: str,
        system_prompt: str = None,
        created_by: str = None,
        description: str = None,
        model_name: str = 'claude-sonnet-4-5-20250514',
        temperature: float = 0.1,
        max_tokens: int = 1024
    ) -> PromptTemplate:
        """
        Create new prompt version (copy-on-edit pattern).

        New versions start as inactive (is_active=False).
        Per USER DECISION: explicit activation required.
        """

    def list_versions(self, task_type: str, name: str) -> list[PromptTemplate]:
        """List all versions for a prompt, ordered by version desc."""
```

Also add helper function:
```python
def get_active_prompt(db: Session, task_type: str, name: str) -> PromptTemplate | None:
    """Convenience function for loading active prompt."""
```

All methods log via structlog for audit trail.
  </action>
  <verify>python -c "from app.services.prompt_manager import PromptVersionManager, get_active_prompt; print('Manager imported')"</verify>
  <done>PromptVersionManager handles activation, rollback to ANY version, version creation with explicit activation</done>
</task>

<task type="auto">
  <name>Task 3: Create PromptMetricsService for performance tracking</name>
  <files>app/services/prompt_metrics_service.py</files>
  <action>
Create PromptMetricsService for extraction-level metrics (REQ-PROMPT-02, REQ-PROMPT-04):

```python
from sqlalchemy.orm import Session
from decimal import Decimal
import structlog

from app.models.prompt_metrics import PromptPerformanceMetrics

logger = structlog.get_logger(__name__)

# Claude API pricing (as of 2026)
CLAUDE_PRICING = {
    'claude-sonnet-4-5-20250514': {'input': 0.003, 'output': 0.015},  # per 1K tokens
    'claude-haiku-4-20250514': {'input': 0.00025, 'output': 0.00125},
}

def calculate_api_cost(
    model_name: str,
    input_tokens: int,
    output_tokens: int
) -> Decimal:
    """
    Calculate API cost in USD based on model and token usage.
    """

def record_extraction_metrics(
    db: Session,
    prompt_template_id: int,
    email_id: int,
    input_tokens: int,
    output_tokens: int,
    model_name: str,
    extraction_success: bool,
    confidence_score: float | None,
    manual_review_required: bool | None,
    execution_time_ms: int
) -> PromptPerformanceMetrics:
    """
    Record extraction-level metrics for a prompt execution.

    REQ-PROMPT-02: Every extraction logs the prompt version used.
    REQ-PROMPT-04: Track tokens, time, success rate per version.

    Args:
        db: Database session
        prompt_template_id: ID of prompt version used
        email_id: IncomingEmail ID being processed
        input_tokens: Input tokens for this API call
        output_tokens: Output tokens from this API call
        model_name: Model used (for cost calculation)
        extraction_success: Did extraction complete successfully?
        confidence_score: Overall confidence (0.0-1.0) or None
        manual_review_required: Was manual review triggered?
        execution_time_ms: Execution time in milliseconds

    Returns:
        Created PromptPerformanceMetrics record
    """

class PromptMetricsService:
    """
    Service wrapper for prompt metrics operations.

    Provides:
    - record_extraction_metrics: Log single extraction
    - get_version_stats: Get aggregated stats for a version
    """

    def __init__(self, db: Session):
        self.db = db

    def record(
        self,
        prompt_template_id: int,
        email_id: int,
        input_tokens: int,
        output_tokens: int,
        model_name: str,
        extraction_success: bool,
        confidence_score: float | None = None,
        manual_review_required: bool | None = None,
        execution_time_ms: int = 0
    ) -> PromptPerformanceMetrics:
        """Record metrics for single extraction."""

    def get_version_stats(
        self,
        prompt_template_id: int,
        days: int = 7
    ) -> dict:
        """
        Get aggregated stats for prompt version over recent days.

        Returns dict with:
        - total_extractions
        - success_rate
        - avg_confidence
        - avg_execution_time_ms
        - total_cost_usd
        """
```
  </action>
  <verify>python -c "from app.services.prompt_metrics_service import PromptMetricsService, record_extraction_metrics, calculate_api_cost; print('Metrics service imported')"</verify>
  <done>PromptMetricsService records extraction metrics with cost calculation, links to prompt_template_id</done>
</task>

</tasks>

<verification>
1. PromptRenderer.render() works with {{ variable }} syntax
2. PromptRenderer.validate_template() catches syntax errors
3. PromptVersionManager.activate_version() atomically switches active version
4. get_active_prompt() returns active version or None
5. record_extraction_metrics() creates PromptPerformanceMetrics with api_cost_usd
</verification>

<success_criteria>
- PromptRenderer renders Jinja2 templates with variable interpolation
- PromptVersionManager handles explicit activation and ANY-version rollback
- get_active_prompt() returns currently active prompt for task_type+name
- PromptMetricsService records metrics linked to prompt_template_id
- calculate_api_cost() returns Decimal cost based on model pricing
</success_criteria>

<output>
After completion, create `.planning/phases/08-database-backed-prompt-management/08-02-SUMMARY.md`
</output>
