---
phase: 08-database-backed-prompt-management
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - app/models/prompt_template.py
  - app/models/prompt_metrics.py
  - app/models/__init__.py
  - alembic/versions/20260206_add_prompt_management.py
autonomous: true

must_haves:
  truths:
    - "PromptTemplate model stores versioned prompts with task_type organization"
    - "PromptPerformanceMetrics tracks per-extraction metrics with 30-day retention"
    - "PromptPerformanceDaily stores aggregated rollups for historical analysis"
    - "Only one version can be active per (task_type, name) pair"
  artifacts:
    - path: "app/models/prompt_template.py"
      provides: "PromptTemplate SQLAlchemy model"
      contains: "class PromptTemplate"
    - path: "app/models/prompt_metrics.py"
      provides: "PromptPerformanceMetrics and PromptPerformanceDaily models"
      contains: "class PromptPerformanceMetrics"
    - path: "alembic/versions/20260206_add_prompt_management.py"
      provides: "Database migration for prompt tables"
      contains: "def upgrade"
  key_links:
    - from: "app/models/prompt_template.py"
      to: "alembic migration"
      via: "SQLAlchemy model defines schema"
      pattern: "class PromptTemplate.*Base"
---

<objective>
Create database models for prompt management: PromptTemplate (versioned templates with task-type organization), PromptPerformanceMetrics (raw extraction-level metrics), and PromptPerformanceDaily (aggregated rollups).

Purpose: Foundation for database-backed prompt storage enabling version tracking, explicit activation, and performance monitoring.
Output: SQLAlchemy models and Alembic migration ready for prompt storage.
</objective>

<execution_context>
@/Users/luka.s/.claude/get-shit-done/workflows/execute-plan.md
@/Users/luka.s/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-database-backed-prompt-management/08-CONTEXT.md
@.planning/phases/08-database-backed-prompt-management/08-RESEARCH.md
@app/models/calibration_sample.py (reference for model conventions)
@app/models/incoming_email.py (reference for model conventions)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create PromptTemplate model with versioning</name>
  <files>app/models/prompt_template.py</files>
  <action>
Create SQLAlchemy model for PromptTemplate with immutable versioning:

```python
class PromptTemplate(Base):
    __tablename__ = "prompt_templates"

    id = Column(Integer, primary_key=True)
    task_type = Column(String(50), nullable=False, index=True)  # 'classification', 'extraction', 'validation'
    name = Column(String(100), nullable=False)  # Human-readable, free-form
    version = Column(Integer, nullable=False)  # Auto-incremented per (task_type, name)

    # Template content (immutable after creation)
    system_prompt = Column(Text, nullable=True)  # Optional system message
    user_prompt_template = Column(Text, nullable=False)  # Jinja2 template

    # Activation state (only one active per task_type + name)
    is_active = Column(Boolean, default=False, nullable=False)

    # Metadata
    created_by = Column(String(100), nullable=True)
    created_at = Column(DateTime(timezone=True), server_default=func.now(), nullable=False)
    description = Column(Text, nullable=True)  # What changed, why

    # Model configuration (part of versioned asset per RESEARCH.md)
    model_name = Column(String(50), default='claude-sonnet-4-5-20250514')
    temperature = Column(Float, default=0.1)
    max_tokens = Column(Integer, default=1024)

    __table_args__ = (
        CheckConstraint('version > 0', name='version_positive'),
        Index('idx_prompt_templates_active', 'task_type', 'name', postgresql_where=text('is_active = TRUE')),
    )
```

USER DECISIONS honored:
- task_type organization (not agent-based)
- Free-form human-readable names
- Model configuration stored with version (Claude's discretion)
- Explicit activation required (is_active=False by default)
  </action>
  <verify>python -c "from app.models.prompt_template import PromptTemplate; print(PromptTemplate.__tablename__)"</verify>
  <done>PromptTemplate model exists with task_type, version, is_active, system_prompt, user_prompt_template, model_name, temperature, max_tokens columns</done>
</task>

<task type="auto">
  <name>Task 2: Create prompt performance metrics models</name>
  <files>app/models/prompt_metrics.py, app/models/__init__.py</files>
  <action>
Create dual-table metrics models per RESEARCH.md Pattern 3:

PromptPerformanceMetrics (raw extraction-level, 30-day retention):
```python
class PromptPerformanceMetrics(Base):
    __tablename__ = "prompt_performance_metrics"

    id = Column(Integer, primary_key=True)
    prompt_template_id = Column(Integer, ForeignKey("prompt_templates.id"), nullable=False, index=True)
    email_id = Column(Integer, ForeignKey("incoming_emails.id"), nullable=False)

    # Cost metrics (USER DECISION: track both cost and quality)
    input_tokens = Column(Integer, nullable=False)
    output_tokens = Column(Integer, nullable=False)
    api_cost_usd = Column(Numeric(10, 6), nullable=False)

    # Quality metrics (USER DECISION: track both cost and quality)
    extraction_success = Column(Boolean, nullable=False)
    confidence_score = Column(Float, nullable=True)
    manual_review_required = Column(Boolean, nullable=True)

    # Execution metrics
    execution_time_ms = Column(Integer, nullable=False)

    extracted_at = Column(DateTime(timezone=True), server_default=func.now(), nullable=False, index=True)
```

PromptPerformanceDaily (aggregated rollups, permanent):
```python
class PromptPerformanceDaily(Base):
    __tablename__ = "prompt_performance_daily"

    id = Column(Integer, primary_key=True)
    prompt_template_id = Column(Integer, ForeignKey("prompt_templates.id"), nullable=False)
    date = Column(Date, nullable=False)

    # Aggregated cost metrics
    total_extractions = Column(Integer, nullable=False)
    total_input_tokens = Column(BigInteger, nullable=False)
    total_output_tokens = Column(BigInteger, nullable=False)
    total_api_cost_usd = Column(Numeric(10, 2), nullable=False)

    # Aggregated quality metrics
    successful_extractions = Column(Integer, nullable=False)
    avg_confidence_score = Column(Float, nullable=True)
    manual_review_count = Column(Integer, nullable=False)

    # Aggregated execution metrics
    avg_execution_time_ms = Column(Integer, nullable=False)
    p95_execution_time_ms = Column(Integer, nullable=True)

    __table_args__ = (
        Index('idx_prompt_daily_unique', 'prompt_template_id', 'date', unique=True),
    )
```

Update app/models/__init__.py to export new models.
  </action>
  <verify>python -c "from app.models import PromptTemplate, PromptPerformanceMetrics, PromptPerformanceDaily; print('Models imported')"</verify>
  <done>Both metrics models exist with cost/quality/execution fields, exported from models package</done>
</task>

<task type="auto">
  <name>Task 3: Create Alembic migration</name>
  <files>alembic/versions/20260206_add_prompt_management.py</files>
  <action>
Create Alembic migration for prompt management tables:

1. Create prompt_templates table with:
   - All columns from PromptTemplate model
   - Partial index on (task_type, name) WHERE is_active = TRUE
   - Index on task_type for filtering
   - Index on created_at for historical queries

2. Create prompt_performance_metrics table with:
   - All columns from PromptPerformanceMetrics model
   - Foreign keys to prompt_templates and incoming_emails
   - Index on prompt_template_id for joins
   - Index on extracted_at for time-range queries

3. Create prompt_performance_daily table with:
   - All columns from PromptPerformanceDaily model
   - Unique constraint on (prompt_template_id, date)
   - Index on date for time-range queries

Migration should follow existing pattern from 20260205_2300_add_calibration_samples.py.

Include downgrade() that drops all three tables in reverse order (daily, metrics, templates).
  </action>
  <verify>cd "/Users/luka.s/NEW AI Creditor Answer Analysis" && python -c "import alembic.versions; print('Migration file valid')" 2>/dev/null || python -c "from alembic.versions.\"20260206_add_prompt_management\" import upgrade, downgrade; print('Migration functions exist')"</verify>
  <done>Migration creates prompt_templates, prompt_performance_metrics, prompt_performance_daily tables with indexes</done>
</task>

</tasks>

<verification>
1. All three models import without errors
2. Migration file has upgrade() and downgrade() functions
3. PromptTemplate has is_active column with default False
4. PromptPerformanceMetrics has foreign keys to prompt_templates and incoming_emails
5. PromptPerformanceDaily has unique constraint on (prompt_template_id, date)
</verification>

<success_criteria>
- PromptTemplate model with task_type, name, version, is_active, system_prompt, user_prompt_template
- PromptPerformanceMetrics model with cost/quality/execution fields and 30-day retention design
- PromptPerformanceDaily model with aggregated fields
- All models exported from app.models package
- Alembic migration ready to run (not executed - deployment handles that)
</success_criteria>

<output>
After completion, create `.planning/phases/08-database-backed-prompt-management/08-01-SUMMARY.md`
</output>
