---
phase: 01-dual-database-audit-consistency
plan: 04
type: execute
wave: 3
depends_on: ["01-02", "01-03"]
files_modified:
  - scripts/audit_consistency.py
  - app/services/audit.py
autonomous: false

must_haves:
  truths:
    - "Audit script connects to both PostgreSQL and MongoDB and compares data"
    - "Audit quantifies: total records in PG, total clients in MongoDB, mismatches by type"
    - "Audit report shows specific mismatches with email IDs, client names, amounts"
    - "Audit produces a recovery plan: which records need re-sync, which need manual review"
    - "User can verify the audit output and confirm the system is working"
  artifacts:
    - path: "scripts/audit_consistency.py"
      provides: "Standalone audit script for one-time data consistency check"
      contains: "def run_audit"
    - path: "app/services/audit.py"
      provides: "AuditService with reusable audit logic"
      contains: "class AuditService"
  key_links:
    - from: "scripts/audit_consistency.py"
      to: "app/services/audit.py"
      via: "CLI script calls AuditService"
      pattern: "AuditService"
    - from: "app/services/audit.py"
      to: "app/services/mongodb_client.py"
      via: "queries MongoDB for comparison"
      pattern: "mongodb_service"
    - from: "app/services/audit.py"
      to: "app/models/incoming_email.py"
      via: "queries IncomingEmail records"
      pattern: "IncomingEmail"
---

<objective>
Create audit script that quantifies existing PostgreSQL-MongoDB mismatches and produces a recovery plan. This fulfills the phase success criterion: "Audit shows existing mismatches quantified with recovery plan."

Purpose: Before the saga pattern processes new emails, we need to understand the current state of data consistency. The v1 system wrote to MongoDB directly without tracking -- this audit reveals how much drift exists and what needs manual attention.

Output: AuditService with comparison logic, CLI script that produces human-readable audit report, checkpoint for user to verify results.
</objective>

<execution_context>
@/Users/luka.s/.claude/get-shit-done/workflows/execute-plan.md
@/Users/luka.s/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-dual-database-audit-consistency/01-RESEARCH.md
@.planning/phases/01-dual-database-audit-consistency/01-02-SUMMARY.md
@.planning/phases/01-dual-database-audit-consistency/01-03-SUMMARY.md

@_existing-code/app/services/mongodb_client.py
@_existing-code/app/models/incoming_email.py
@_existing-code/app/database.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create AuditService and CLI audit script</name>
  <files>
    app/services/audit.py
    scripts/audit_consistency.py
  </files>
  <action>
    1. Create `app/services/audit.py` with AuditService class:

    ```python
    class AuditService:
        """
        One-time audit comparing PostgreSQL incoming_emails
        with MongoDB clients.final_creditor_list data.
        """

        def __init__(self, session_factory, mongodb_service):
            self.session_factory = session_factory
            self.mongodb_service = mongodb_service

        def run_full_audit(self, lookback_days: int = 30) -> dict:
            """
            Compare PostgreSQL and MongoDB data for the last N days.

            Returns audit report dict:
            {
                "summary": {
                    "pg_total_emails": N,
                    "pg_completed_emails": N,
                    "pg_matched_emails": N,
                    "mongo_total_clients": N,
                    "mongo_clients_with_responses": N,
                    "audit_period_days": 30,
                    "audit_timestamp": "..."
                },
                "mismatches": [
                    {
                        "type": "pg_matched_no_mongo_record",
                        "email_id": N,
                        "client_name": "...",
                        "creditor_name": "...",
                        "debt_amount": N,
                        "severity": "high",
                        "recovery_action": "re-sync from PostgreSQL extracted_data"
                    },
                    ...
                ],
                "recovery_plan": {
                    "auto_recoverable": N,
                    "manual_review_needed": N,
                    "no_action_needed": N,
                    "actions": [
                        {"email_id": N, "action": "re-sync", "reason": "..."},
                        {"email_id": N, "action": "manual_review", "reason": "..."},
                    ]
                },
                "health_score": 0.0-1.0  # (total - mismatches) / total
            }
            ```

            Audit checks:
            a) For each IncomingEmail with match_status='auto_matched' and processing_status='completed':
               - Does MongoDB have matching client + creditor with updated amount?
               - Compare extracted_data.debt_amount with MongoDB creditor_response_amount
               - Flag mismatches

            b) Count IncomingEmail records by processing_status (received, parsed, extracted, matched, completed, failed)
               - 'failed' records need investigation
               - 'received'/'parsed' records that are >24 hours old indicate stalled processing

            c) Count MongoDB clients with creditor responses that have NO corresponding IncomingEmail
               - These may be from the Node.js portal or manual entries -- classify as "expected" or "unexpected"

            d) Generate recovery plan:
               - "re-sync": PG has data, MongoDB doesn't -- can auto-fix
               - "manual_review": Both have data but it differs -- needs human decision
               - "stalled": PG record stuck in intermediate status -- needs retry or investigation
               - "no_action": Data matches, everything consistent

        def _find_mongo_match(self, client_name, creditor_name, creditor_email) -> Optional[dict]:
            """
            Search MongoDB for a client with matching creditor in final_creditor_list.
            Uses same matching logic as mongodb_client.py (aktenzeichen > name search).
            Returns the creditor entry dict if found, None otherwise.
            """
    ```

    2. Create `scripts/audit_consistency.py` as standalone CLI script:

    ```python
    #!/usr/bin/env python3
    """
    One-time data consistency audit between PostgreSQL and MongoDB.
    Run: python scripts/audit_consistency.py [--lookback-days 30]

    Produces human-readable report of mismatches and recovery plan.
    """
    import argparse
    import json
    import sys
    from datetime import datetime

    # Add project root to path
    sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

    from app.database import init_db, SessionLocal
    from app.services.mongodb_client import mongodb_service
    from app.services.audit import AuditService
    ```

    The script should:
    a) Parse --lookback-days argument (default 30)
    b) Initialize database connections
    c) Run AuditService.run_full_audit()
    d) Print formatted report to stdout:
       - Summary section with counts
       - Mismatches table (email_id, type, severity, details)
       - Recovery plan with recommended actions
       - Overall health score
    e) Save JSON report to `scripts/audit_report_{timestamp}.json`
    f) Exit with code 0 if health_score >= 0.95, exit code 1 otherwise

    IMPORTANT: The script must work WITHOUT running the FastAPI app. It initializes DB connections directly.
    IMPORTANT: Handle the case where MongoDB is not available gracefully (report it, don't crash).
    IMPORTANT: Handle the case where PostgreSQL has no incoming_emails (new installation) -- report "clean" with 0 mismatches.
  </action>
  <verify>
    python -c "from app.services.audit import AuditService; print('AuditService imports successfully')"
    python scripts/audit_consistency.py --help (should show usage without crashing)
  </verify>
  <done>
    AuditService compares PostgreSQL incoming_emails with MongoDB clients.final_creditor_list. CLI script runs standalone, produces formatted report with summary/mismatches/recovery plan/health score. JSON report saved to file. Exit code reflects health score. Handles missing databases gracefully.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
    Complete dual-database saga infrastructure for Phase 1:
    1. Database models: OutboxMessage, IdempotencyKey, ReconciliationReport + IncomingEmail sync columns (Plan 01)
    2. DualDatabaseWriter with saga pattern and idempotency (Plan 02)
    3. Webhook refactored to use saga-based dual writes (Plan 02)
    4. ReconciliationService with hourly APScheduler job (Plan 03)
    5. AuditService with CLI audit script (Plan 04)
  </what-built>
  <how-to-verify>
    1. Review the audit script output: `python scripts/audit_consistency.py --lookback-days 30`
       - Should show summary of PostgreSQL and MongoDB records
       - Should list any mismatches found
       - Should show recovery plan with recommended actions
       - Should show health score

    2. Review the codebase structure:
       - app/models/ should have outbox_message.py, idempotency_key.py, reconciliation_report.py
       - app/services/ should have dual_write.py, idempotency.py, reconciliation.py, audit.py
       - scripts/ should have audit_consistency.py
       - alembic/versions/ should have saga infrastructure migration

    3. Test manual reconciliation trigger (if app is running):
       - POST to /api/v1/admin/reconciliation/trigger
       - Should return reconciliation results

    4. Verify requirements.txt has structlog and apscheduler but NOT celery
  </how-to-verify>
  <resume-signal>Type "approved" if the audit report and code structure look correct, or describe any issues found.</resume-signal>
</task>

</tasks>

<verification>
1. `python -c "from app.services.audit import AuditService"` succeeds
2. `python scripts/audit_consistency.py --help` shows usage
3. Audit script connects to both databases and produces report
4. Recovery plan categorizes mismatches into auto-recoverable vs manual review
5. All Phase 1 components work together: models -> dual_write -> reconciliation -> audit
</verification>

<success_criteria>
- Audit script runs standalone and produces formatted report
- Existing mismatches between PostgreSQL and MongoDB are quantified
- Recovery plan generated with specific actions per mismatch
- Health score calculated (ratio of consistent to total records)
- JSON report saved to file for reference
- User has verified the complete Phase 1 infrastructure
</success_criteria>

<output>
After completion, create `.planning/phases/01-dual-database-audit-consistency/01-04-SUMMARY.md`
</output>
