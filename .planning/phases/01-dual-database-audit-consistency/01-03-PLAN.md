---
phase: 01-dual-database-audit-consistency
plan: 03
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - app/services/reconciliation.py
  - app/main.py
  - requirements.txt
autonomous: true

must_haves:
  truths:
    - "Reconciliation job runs hourly comparing PostgreSQL incoming_emails with MongoDB clients collection"
    - "Mismatches are detected: records in PostgreSQL with sync_status=synced but missing/different in MongoDB"
    - "Auto-repair attempts to re-sync missing MongoDB writes from PostgreSQL data"
    - "ReconciliationReport records each run with counts of checked, mismatches, repaired, failed"
    - "Expired idempotency keys and old processed outbox messages are cleaned up"
    - "Pending outbox messages (failed MongoDB writes) are retried during reconciliation"
  artifacts:
    - path: "app/services/reconciliation.py"
      provides: "ReconciliationService with compare, repair, and scheduled run logic"
      contains: "class ReconciliationService"
    - path: "app/main.py"
      provides: "APScheduler startup hook for hourly reconciliation"
      contains: "scheduler"
  key_links:
    - from: "app/services/reconciliation.py"
      to: "app/models/reconciliation_report.py"
      via: "creates ReconciliationReport records"
      pattern: "ReconciliationReport"
    - from: "app/services/reconciliation.py"
      to: "app/services/mongodb_client.py"
      via: "queries MongoDB for comparison"
      pattern: "mongodb_service"
    - from: "app/services/reconciliation.py"
      to: "app/services/dual_write.py"
      via: "retries pending outbox messages"
      pattern: "process_pending_outbox"
    - from: "app/main.py"
      to: "app/services/reconciliation.py"
      via: "APScheduler calls run_reconciliation hourly"
      pattern: "scheduler.add_job"
---

<objective>
Implement hourly reconciliation job that compares PostgreSQL (source of truth) with MongoDB, detects mismatches, auto-repairs where possible, and produces audit reports. Uses APScheduler for scheduling instead of Celery (Phase 2 introduces Dramatiq, so no Celery).

Purpose: Reconciliation is the safety net for the saga pattern. Even with perfect dual-write logic, edge cases (process crashes, network partitions) can cause PostgreSQL-MongoDB drift. This catches and repairs those inconsistencies hourly.

Output: ReconciliationService with comparison/repair logic, APScheduler integration in FastAPI startup.
</objective>

<execution_context>
@/Users/luka.s/.claude/get-shit-done/workflows/execute-plan.md
@/Users/luka.s/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-dual-database-audit-consistency/01-RESEARCH.md
@.planning/phases/01-dual-database-audit-consistency/01-01-SUMMARY.md

@_existing-code/app/main.py
@_existing-code/app/services/mongodb_client.py
@_existing-code/app/database.py
@_existing-code/app/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ReconciliationService with comparison and repair logic</name>
  <files>
    app/services/reconciliation.py
  </files>
  <action>
    Create `app/services/reconciliation.py` with ReconciliationService class.

    The reconciliation job has THREE responsibilities per run:
    A) Retry pending outbox messages (failed MongoDB writes)
    B) Compare PostgreSQL with MongoDB for recent records
    C) Cleanup expired idempotency keys and old outbox messages

    Class design:

    ```python
    class ReconciliationService:
        def __init__(self, session_factory, mongodb_service):
            """
            Args:
                session_factory: SQLAlchemy sessionmaker for creating independent sessions
                mongodb_service: MongoDBService instance for MongoDB queries
            """

        def run_reconciliation(self) -> dict:
            """
            Main entry point. Runs full reconciliation cycle.
            Returns dict with summary stats.
            Creates ReconciliationReport record.
            """
            # 1. Create ReconciliationReport with status='running'
            # 2. Call _retry_pending_outbox()
            # 3. Call _compare_recent_records()
            # 4. Call _cleanup_stale_data()
            # 5. Update ReconciliationReport with results, status='completed'
            # 6. Return summary dict

        def _retry_pending_outbox(self, session) -> dict:
            """
            Find OutboxMessage records where processed_at IS NULL
            and retry_count < max_retries. Attempt MongoDB write for each.
            Returns {"retried": N, "succeeded": N, "failed": N}
            """
            # Query unprocessed outbox messages
            # For each: load payload, call mongodb_service.update_creditor_debt_amount()
            # On success: mark processed_at, update IncomingEmail.sync_status='synced'
            # On failure: increment retry_count, store error

        def _compare_recent_records(self, session, lookback_hours: int = 48) -> list:
            """
            Compare PostgreSQL incoming_emails (last 48 hours, sync_status='synced')
            with MongoDB to detect drift.

            For each synced email with extracted_data containing debt_amount:
            1. Find the client in MongoDB by client_name/aktenzeichen
            2. Find the creditor in final_creditor_list
            3. Compare creditor_response_amount with PostgreSQL extracted_data.debt_amount
            4. If mismatch: record it, attempt repair (update MongoDB from PG data)

            Returns list of mismatch dicts:
            [{"type": "missing_in_mongo"|"data_mismatch"|"extra_in_mongo",
              "email_id": N, "field": "...", "pg_value": ..., "mongo_value": ...,
              "repaired": bool}]
            """

        def _cleanup_stale_data(self, session) -> dict:
            """
            1. Delete IdempotencyKey records where expires_at < now()
            2. Delete OutboxMessage records where processed_at is not null
               AND created_at < now() - 30 days (cleanup old processed messages)
            Returns {"expired_keys_deleted": N, "old_outbox_deleted": N}
            """

        def _repair_missing_in_mongo(self, session, email, extracted_data) -> bool:
            """
            Attempt to write missing data to MongoDB from PostgreSQL.
            Uses mongodb_service.update_creditor_debt_amount() with data
            from IncomingEmail.extracted_data.
            Returns True if repair succeeded.
            """
    ```

    IMPORTANT: Each reconciliation run creates its OWN SQLAlchemy session (from session_factory). Do NOT reuse sessions across the entire run -- create one per run and close it in finally block.
    IMPORTANT: Use structlog for all logging with context: reconciliation_run_id, step, counts.
    IMPORTANT: Wrap the entire run in try/except. If reconciliation crashes, update ReconciliationReport.status='failed' with error message. Never let a crash go unrecorded.
    IMPORTANT: The comparison logic must handle the case where MongoDB is unavailable (mongodb_service.is_available() returns False) -- log warning and skip comparison, but still do outbox retry and cleanup.
  </action>
  <verify>
    python -c "from app.services.reconciliation import ReconciliationService; print('ReconciliationService imports successfully')"
  </verify>
  <done>
    ReconciliationService has run_reconciliation() that: retries pending outbox messages, compares PG with MongoDB for last 48 hours of synced records, cleans up expired idempotency keys and old outbox messages. Each run produces a ReconciliationReport record with status/counts/details. Handles MongoDB unavailability gracefully.
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate APScheduler for hourly reconciliation in FastAPI</name>
  <files>
    app/main.py
    requirements.txt
  </files>
  <action>
    1. Add `apscheduler>=3.10.0` to requirements.txt (lightweight scheduler, no separate worker process needed).
       Do NOT add celery. Phase 2 introduces Dramatiq which will replace APScheduler for task scheduling.

    2. Modify `app/main.py` to start APScheduler on application startup:

    ```python
    from apscheduler.schedulers.background import BackgroundScheduler
    from apscheduler.triggers.interval import IntervalTrigger
    from app.services.reconciliation import ReconciliationService
    from app.services.mongodb_client import mongodb_service
    import structlog

    logger = structlog.get_logger()

    # Scheduler instance (module level)
    scheduler = BackgroundScheduler(timezone="Europe/Berlin")

    def run_scheduled_reconciliation():
        """Wrapper for scheduled reconciliation job"""
        try:
            from app.database import SessionLocal
            if SessionLocal is None:
                logger.warning("reconciliation_skipped", reason="database_not_configured")
                return

            reconciliation_svc = ReconciliationService(
                session_factory=SessionLocal,
                mongodb_service=mongodb_service
            )
            result = reconciliation_svc.run_reconciliation()
            logger.info("reconciliation_completed", **result)
        except Exception as e:
            logger.error("reconciliation_crashed", error=str(e), exc_info=True)
    ```

    3. In the existing `startup_event()` function, AFTER `init_db()`, add:
    ```python
    # Start reconciliation scheduler
    if settings.environment != "testing":
        scheduler.add_job(
            run_scheduled_reconciliation,
            trigger=IntervalTrigger(hours=1),
            id="hourly_reconciliation",
            name="Hourly PostgreSQL-MongoDB Reconciliation",
            replace_existing=True
        )
        scheduler.start()
        logger.info("Reconciliation scheduler started (hourly)")
    ```

    4. In the existing `shutdown_event()` function, add:
    ```python
    if scheduler.running:
        scheduler.shutdown(wait=False)
        logger.info("Reconciliation scheduler stopped")
    ```

    5. Add a manual trigger endpoint for testing/operations:
    ```python
    @app.post("/api/v1/admin/reconciliation/trigger")
    async def trigger_reconciliation():
        """Manually trigger reconciliation (for testing and ops)"""
        from app.database import SessionLocal
        if SessionLocal is None:
            return {"status": "error", "message": "Database not configured"}

        reconciliation_svc = ReconciliationService(
            session_factory=SessionLocal,
            mongodb_service=mongodb_service
        )
        result = reconciliation_svc.run_reconciliation()
        return {"status": "completed", "result": result}
    ```

    IMPORTANT: Use `BackgroundScheduler` (not `AsyncIOScheduler`) because reconciliation uses synchronous SQLAlchemy and PyMongo. BackgroundScheduler runs in its own thread, which is fine for hourly jobs.
    IMPORTANT: Skip scheduler in testing environment to avoid interference with tests.
    IMPORTANT: Keep all existing main.py functionality (routers, health check, root endpoint).
    IMPORTANT: The `@app.on_event("startup")` and `@app.on_event("shutdown")` decorators are the existing pattern -- keep using them (not lifespan). Phase 9 may modernize to lifespan pattern.
  </action>
  <verify>
    python -c "from app.main import app, scheduler; print(f'App created, scheduler type: {type(scheduler).__name__}')"

    Verify main.py contains:
    - APScheduler import and BackgroundScheduler instantiation
    - scheduler.add_job() in startup_event
    - scheduler.shutdown() in shutdown_event
    - /api/v1/admin/reconciliation/trigger endpoint
  </verify>
  <done>
    APScheduler runs hourly reconciliation job inside FastAPI process. Reconciliation retries failed outbox messages, compares PG-MongoDB for recent records, cleans up expired keys. Manual trigger endpoint available at /api/v1/admin/reconciliation/trigger. Scheduler skipped in testing environment. No Celery dependency introduced.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from app.services.reconciliation import ReconciliationService"` succeeds
2. `python -c "from app.main import app, scheduler"` succeeds
3. requirements.txt contains apscheduler but NOT celery
4. main.py has scheduler startup and shutdown hooks
5. Manual reconciliation trigger endpoint exists
</verification>

<success_criteria>
- ReconciliationService compares PostgreSQL with MongoDB, detects mismatches, auto-repairs
- APScheduler runs reconciliation hourly inside FastAPI process
- Manual trigger endpoint available for operations
- ReconciliationReport records produced for each run
- Expired idempotency keys and old outbox messages cleaned up
- No Celery dependency (Phase 2 introduces Dramatiq)
</success_criteria>

<output>
After completion, create `.planning/phases/01-dual-database-audit-consistency/01-03-SUMMARY.md`
</output>
