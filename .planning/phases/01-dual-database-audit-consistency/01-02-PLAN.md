---
phase: 01-dual-database-audit-consistency
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - app/services/idempotency.py
  - app/services/dual_write.py
  - app/routers/webhook.py
  - requirements.txt
autonomous: true

must_haves:
  truths:
    - "PostgreSQL write commits first as source of truth, then MongoDB write is attempted"
    - "If MongoDB write fails, PostgreSQL record is marked pending_mongodb_sync for later reconciliation"
    - "Duplicate webhook requests with same idempotency key are rejected without re-processing"
    - "Outbox message is created in same PostgreSQL transaction as business data write"
    - "Webhook endpoint returns 200 quickly while saga runs in background task"
  artifacts:
    - path: "app/services/idempotency.py"
      provides: "PostgreSQL-backed idempotency checking and storage"
      contains: "class IdempotencyService"
    - path: "app/services/dual_write.py"
      provides: "DualDatabaseWriter with saga pattern orchestration"
      contains: "class DualDatabaseWriter"
    - path: "app/routers/webhook.py"
      provides: "Refactored webhook using DualDatabaseWriter instead of direct MongoDB writes"
      contains: "DualDatabaseWriter"
  key_links:
    - from: "app/routers/webhook.py"
      to: "app/services/dual_write.py"
      via: "process_incoming_email calls DualDatabaseWriter"
      pattern: "DualDatabaseWriter"
    - from: "app/services/dual_write.py"
      to: "app/services/idempotency.py"
      via: "checks idempotency before writing"
      pattern: "IdempotencyService"
    - from: "app/services/dual_write.py"
      to: "app/models/outbox_message.py"
      via: "creates OutboxMessage in same transaction"
      pattern: "OutboxMessage"
    - from: "app/services/dual_write.py"
      to: "app/services/mongodb_client.py"
      via: "calls mongodb_service for MongoDB writes"
      pattern: "mongodb_service"
---

<objective>
Implement the core saga pattern: DualDatabaseWriter that writes to PostgreSQL first (source of truth), creates outbox messages atomically, then attempts MongoDB write with compensation on failure. Refactor the webhook to use this instead of direct MongoDB writes.

Purpose: This is the heart of Phase 1 -- ensuring every database write goes through the saga pattern so PostgreSQL and MongoDB stay consistent. The existing webhook writes to MongoDB directly without any safety net; this plan replaces that with the transactional outbox pattern.

Output: IdempotencyService, DualDatabaseWriter, and refactored webhook that uses saga-based dual writes.
</objective>

<execution_context>
@/Users/luka.s/.claude/get-shit-done/workflows/execute-plan.md
@/Users/luka.s/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-dual-database-audit-consistency/01-RESEARCH.md
@.planning/phases/01-dual-database-audit-consistency/01-01-SUMMARY.md

@_existing-code/app/routers/webhook.py
@_existing-code/app/services/mongodb_client.py
@_existing-code/app/database.py
@_existing-code/app/config.py
@_existing-code/requirements.txt
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create IdempotencyService and DualDatabaseWriter</name>
  <files>
    app/services/idempotency.py
    app/services/dual_write.py
    requirements.txt
  </files>
  <action>
    1. Add `structlog>=24.1.0` to requirements.txt (for structured logging in saga operations). Do NOT add celery or redis -- those come in Phase 2.

    2. Create `app/services/idempotency.py` with IdempotencyService class:
       - `__init__(self, session_factory)`: Takes SQLAlchemy sessionmaker (not a session -- creates its own for independent transactions)
       - `check(self, key: str) -> Optional[dict]`: Query IdempotencyKey table. If key exists AND not expired (expires_at > now), return cached result. If expired, delete it and return None.
       - `store(self, key: str, result: dict, ttl_seconds: int = 86400)`: Insert IdempotencyKey with expires_at = now + ttl_seconds. Use `INSERT ... ON CONFLICT DO NOTHING` pattern to handle race conditions.
       - `cleanup_expired(self)`: Delete all IdempotencyKey rows where expires_at < now. Called by reconciliation job.
       - Generate idempotency keys using: `f"{operation_type}:{aggregate_id}:{content_hash[:16]}"` where content_hash is SHA256 of JSON-serialized payload.
       - Include a module-level function `generate_idempotency_key(operation: str, aggregate_id: str, payload: dict) -> str` for consistent key generation.

    3. Create `app/services/dual_write.py` with DualDatabaseWriter class:
       - `__init__(self, pg_session: Session, idempotency_service: IdempotencyService)`: Accepts active SQLAlchemy session and idempotency service.
       - `update_creditor_debt(self, email_id: int, client_name: str, client_aktenzeichen: Optional[str], creditor_email: str, creditor_name: str, new_debt_amount: float, response_text: Optional[str], reference_numbers: Optional[list], idempotency_key: str) -> dict`:
         a. Check idempotency -- if already processed, return cached result
         b. Create OutboxMessage in same session (same PG transaction as any business data updates)
         c. Update IncomingEmail.sync_status to 'pending' and set idempotency_key
         d. Flush session (do NOT commit -- caller controls transaction)
         e. Return dict with outbox_message_id for post-commit MongoDB write

       - `execute_mongodb_write(self, outbox_message_id: int) -> bool`:
         a. Load OutboxMessage by ID
         b. Call mongodb_service.update_creditor_debt_amount() with payload from outbox message
         c. If success: mark outbox_message.processed_at = now, update IncomingEmail.sync_status = 'synced'
         d. If failure: increment outbox_message.retry_count, store error in outbox_message.error_message, set IncomingEmail.sync_status = 'failed', IncomingEmail.sync_error = error message
         e. Commit changes
         f. Return success boolean

       - `process_pending_outbox(self) -> int`:
         a. Query OutboxMessage where processed_at IS NULL and retry_count < max_retries, ordered by created_at
         b. For each: call execute_mongodb_write
         c. Return count of successfully processed messages
         d. This method is called by the reconciliation scheduler to retry failed MongoDB writes

       Use structlog for all logging with context fields: email_id, idempotency_key, operation, saga_step.

       IMPORTANT: Import mongodb_service from app.services.mongodb_client (the existing singleton).
       IMPORTANT: The DualDatabaseWriter does NOT control the PostgreSQL transaction commit for the initial write -- the caller (webhook) does. This ensures the outbox message is atomic with the business data.
       IMPORTANT: execute_mongodb_write creates its own session for updating outbox status after MongoDB operation.
  </action>
  <verify>
    python -c "from app.services.idempotency import IdempotencyService, generate_idempotency_key; from app.services.dual_write import DualDatabaseWriter; print('Services import successfully')"
  </verify>
  <done>
    IdempotencyService provides check/store/cleanup_expired with PostgreSQL-backed storage. DualDatabaseWriter provides update_creditor_debt (atomic PG write + outbox) and execute_mongodb_write (compensatable MongoDB operation). Both use structlog for structured logging with saga context.
  </done>
</task>

<task type="auto">
  <name>Task 2: Refactor webhook to use DualDatabaseWriter saga pattern</name>
  <files>
    app/routers/webhook.py
  </files>
  <action>
    Refactor the `process_incoming_email` background task in webhook.py to use DualDatabaseWriter instead of calling mongodb_service directly.

    Current flow (lines ~244-334 of existing webhook.py):
    ```
    # Step 3: Match directly in MongoDB (skip PostgreSQL matching)
    mongodb_update_success = mongodb_service.update_creditor_debt_amount(...)
    ```

    New flow:
    1. After entity extraction succeeds (Step 2 in existing code), generate idempotency key:
       ```python
       from app.services.idempotency import IdempotencyService, generate_idempotency_key
       from app.services.dual_write import DualDatabaseWriter

       idempotency_key = generate_idempotency_key(
           operation="creditor_debt_update",
           aggregate_id=str(email_id),
           payload={"client_name": client_name, "creditor_email": creditor_email, "amount": new_debt_amount}
       )
       ```

    2. Create DualDatabaseWriter with current session and idempotency service:
       ```python
       idempotency_svc = IdempotencyService(SessionLocal)
       dual_writer = DualDatabaseWriter(db, idempotency_svc)
       ```

    3. Replace the direct mongodb_service.update_creditor_debt_amount() call with:
       ```python
       result = dual_writer.update_creditor_debt(
           email_id=email_id,
           client_name=client_name,
           client_aktenzeichen=client_aktenzeichen,
           creditor_email=creditor_email,
           creditor_name=creditor_name_or_email,
           new_debt_amount=new_debt_amount,
           response_text=extracted_entities.summary,
           reference_numbers=extracted_entities.reference_numbers,
           idempotency_key=idempotency_key
       )
       db.commit()  # Commit PG transaction (outbox message included atomically)

       # Now attempt MongoDB write (post-commit, compensatable)
       if result.get("outbox_message_id"):
           mongodb_success = dual_writer.execute_mongodb_write(result["outbox_message_id"])
       ```

    4. Keep the existing email_notifier.send_debt_update_notification() call -- trigger it only when mongodb_success is True (same behavior as before).

    5. Keep all existing status updates (email.match_status, email.processing_status) -- they still work the same way.

    6. The "MongoDB-only mode" path (when db is None) should remain unchanged -- it is a fallback for when PostgreSQL is not configured. Do NOT break this path.

    IMPORTANT: Do NOT remove any existing functionality. The webhook must still:
    - Verify webhook signature
    - Deduplicate by webhook_id
    - Parse and store email
    - Extract entities with LLM
    - Check is_creditor_reply
    - Send email notification on success
    - Update processing_status throughout

    IMPORTANT: Do NOT import or use Celery/Dramatiq. The background task still uses FastAPI BackgroundTasks (existing pattern). Phase 2 replaces this with Dramatiq.
  </action>
  <verify>
    python -c "from app.routers.webhook import router; print('Webhook router imports successfully')"

    Verify webhook.py contains:
    - Import of DualDatabaseWriter and IdempotencyService
    - Call to dual_writer.update_creditor_debt() instead of mongodb_service.update_creditor_debt_amount()
    - Call to dual_writer.execute_mongodb_write() after db.commit()
    - Preserved email_notifier call
    - Preserved MongoDB-only fallback path
  </verify>
  <done>
    Webhook process_incoming_email uses DualDatabaseWriter for all creditor debt updates. PostgreSQL writes commit first with outbox message atomically. MongoDB write attempted post-commit with compensation on failure. Idempotency prevents duplicate processing. Email notifications still sent on successful MongoDB write. MongoDB-only fallback path preserved for backward compatibility.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from app.services.dual_write import DualDatabaseWriter; from app.services.idempotency import IdempotencyService"` succeeds
2. `python -c "from app.routers.webhook import router"` succeeds
3. webhook.py no longer calls mongodb_service.update_creditor_debt_amount() directly (only via DualDatabaseWriter)
4. structlog is in requirements.txt
5. No celery or redis in requirements.txt
</verification>

<success_criteria>
- DualDatabaseWriter implements saga pattern: PG write + outbox (atomic) then MongoDB write (compensatable)
- IdempotencyService prevents duplicate operations using PostgreSQL-backed keys
- Webhook uses DualDatabaseWriter instead of direct MongoDB writes
- All existing webhook behavior preserved (signature verification, dedup, extraction, notification)
- MongoDB-only fallback path unchanged
- structlog added to requirements.txt (no celery/redis)
</success_criteria>

<output>
After completion, create `.planning/phases/01-dual-database-audit-consistency/01-02-SUMMARY.md`
</output>
