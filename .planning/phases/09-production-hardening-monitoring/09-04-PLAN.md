---
phase: 09-production-hardening-monitoring
plan: 04
type: execute
wave: 3
depends_on: ["09-01", "09-03"]
files_modified:
  - app/services/monitoring/error_tracking.py
  - app/services/monitoring/__init__.py
  - app/models/processing_report.py
  - app/models/__init__.py
  - app/services/processing_reports.py
  - alembic/versions/xxx_add_processing_reports.py
  - requirements.txt
autonomous: true

must_haves:
  truths:
    - "Sentry captures errors with email_id, job_id, actor context"
    - "Processing reports show per-email: what extracted, what missing, confidence per field"
    - "Processing reports queryable in database"
  artifacts:
    - path: "app/services/monitoring/error_tracking.py"
      provides: "Sentry context helpers"
      contains: "set_processing_context"
    - path: "app/models/processing_report.py"
      provides: "Per-email processing report model"
      contains: "ProcessingReport"
    - path: "app/services/processing_reports.py"
      provides: "Report generation service"
      contains: "create_processing_report"
  key_links:
    - from: "app/services/monitoring/error_tracking.py"
      to: "sentry-sdk"
      via: "context setting"
      pattern: "sentry_sdk.set_context"
    - from: "app/services/processing_reports.py"
      to: "app/models/processing_report.py"
      via: "SQLAlchemy model"
      pattern: "ProcessingReport"
---

<objective>
Implement Sentry error tracking with rich context and processing reports that capture per-email extraction results.

Purpose: Sentry integration enables investigation of production errors with full context. Processing reports satisfy REQ-OPS-06 by providing visibility into extraction results per email.

Output: Sentry context helpers, ProcessingReport model, and report generation service.
</objective>

<execution_context>
@/Users/luka.s/.claude/get-shit-done/workflows/execute-plan.md
@/Users/luka.s/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-production-hardening-monitoring/09-CONTEXT.md
@.planning/phases/09-production-hardening-monitoring/09-RESEARCH.md

# Existing patterns
@app/models/incoming_email.py
@app/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add Sentry SDK and create error tracking helpers</name>
  <files>
    requirements.txt
    app/services/monitoring/error_tracking.py
    app/services/monitoring/__init__.py
    app/config.py
  </files>
  <action>
    1. Add to requirements.txt:
       - sentry-sdk[fastapi]>=2.0.0

    2. Add to app/config.py Settings class:
       - sentry_dsn: Optional[str] = None  # Sentry project DSN
       - sentry_environment: Optional[str] = None  # Defaults to environment setting

    3. Create app/services/monitoring/error_tracking.py:

       a. init_sentry() function:
          - If settings.sentry_dsn is None, log warning and return (disabled)
          - Import sentry_sdk and sentry_sdk.integrations.fastapi
          - Call sentry_sdk.init(
              dsn=settings.sentry_dsn,
              environment=settings.sentry_environment or settings.environment,
              traces_sample_rate=0.1,  # 10% of requests traced
              profiles_sample_rate=0.1,
              integrations=[FastApiIntegration()]
            )
          - Log "Sentry initialized"

       b. set_processing_context(email_id: int, actor: str, correlation_id: str = None):
          - Set Sentry context for current processing:
            ```python
            sentry_sdk.set_context("processing", {
                "email_id": email_id,
                "actor": actor,
                "correlation_id": correlation_id or "none"
            })
            sentry_sdk.set_tag("email_id", str(email_id))
            sentry_sdk.set_tag("actor", actor)
            ```

       c. add_breadcrumb(category: str, message: str, level: str = "info", data: dict = None):
          - Wrapper around sentry_sdk.add_breadcrumb for consistent usage:
            ```python
            sentry_sdk.add_breadcrumb(
                category=category,
                message=message,
                level=level,
                data=data or {}
            )
            ```

       d. capture_message(message: str, level: str = "info"):
          - Wrapper for sentry_sdk.capture_message
          - Useful for non-exception events worth tracking

    4. Update app/services/monitoring/__init__.py:
       - Export init_sentry, set_processing_context, add_breadcrumb, capture_message
  </action>
  <verify>
    pip install "sentry-sdk[fastapi]>=2.0.0" && python -c "from app.services.monitoring.error_tracking import init_sentry, set_processing_context; print('Error tracking imports successfully')"
  </verify>
  <done>
    sentry-sdk added to requirements.txt.
    init_sentry() initializes Sentry with FastAPI integration.
    set_processing_context() sets email_id, actor, correlation_id for error context.
    Helper functions wrap Sentry SDK for consistent usage.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create ProcessingReport model and migration</name>
  <files>
    app/models/processing_report.py
    app/models/__init__.py
    alembic/versions/xxx_add_processing_reports.py
  </files>
  <action>
    1. Create app/models/processing_report.py:

       ProcessingReport(Base) - Per-email processing audit trail:
       - id: Integer, primary key
       - email_id: Integer, ForeignKey("incoming_emails.id"), unique, nullable=False
       - created_at: DateTime(timezone=True), server_default=func.now()

       # Extraction results (what was extracted)
       - extracted_fields: JSON, nullable=False
         - Structure: {"client_name": {"value": "...", "confidence": 0.9, "source": "email_body"},
                       "creditor_name": {...}, "debt_amount": {...}, "reference_numbers": {...}}

       # Missing fields (what's missing)
       - missing_fields: JSON, nullable=True
         - List of field names that couldn't be extracted: ["reference_numbers"]

       # Overall assessment
       - overall_confidence: Float, nullable=False
       - confidence_route: String(20), nullable=False  # "high", "medium", "low"
       - needs_review: Boolean, default=False
       - review_reason: String(100), nullable=True

       # Pipeline metadata
       - intent: String(50), nullable=True  # debt_statement, payment_plan, etc.
       - sources_processed: Integer, default=1
       - total_tokens_used: Integer, default=0
       - processing_time_ms: Integer, nullable=True

       __table_args__: Index on email_id (already unique), index on created_at for date queries

    2. Update app/models/__init__.py:
       - Import and export ProcessingReport

    3. Create Alembic migration:
       ```python
       """Add processing reports table

       Revision ID: [generate]
       Revises: [operational_metrics migration]
       """
       from alembic import op
       import sqlalchemy as sa
       from sqlalchemy.dialects.postgresql import JSON

       def upgrade() -> None:
           op.create_table(
               'processing_reports',
               sa.Column('id', sa.Integer(), nullable=False),
               sa.Column('email_id', sa.Integer(), nullable=False),
               sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.func.now()),
               sa.Column('extracted_fields', JSON(), nullable=False),
               sa.Column('missing_fields', JSON(), nullable=True),
               sa.Column('overall_confidence', sa.Float(), nullable=False),
               sa.Column('confidence_route', sa.String(20), nullable=False),
               sa.Column('needs_review', sa.Boolean(), default=False),
               sa.Column('review_reason', sa.String(100), nullable=True),
               sa.Column('intent', sa.String(50), nullable=True),
               sa.Column('sources_processed', sa.Integer(), default=1),
               sa.Column('total_tokens_used', sa.Integer(), default=0),
               sa.Column('processing_time_ms', sa.Integer(), nullable=True),
               sa.PrimaryKeyConstraint('id'),
               sa.ForeignKeyConstraint(['email_id'], ['incoming_emails.id']),
               sa.UniqueConstraint('email_id', name='uq_processing_report_email')
           )
           op.create_index('idx_processing_report_created', 'processing_reports', ['created_at'])

       def downgrade() -> None:
           op.drop_table('processing_reports')
       ```
  </action>
  <verify>
    python -c "from app.models.processing_report import ProcessingReport; print('ProcessingReport model defined')"
  </verify>
  <done>
    ProcessingReport model captures per-email extraction details.
    Stores: extracted_fields with per-field confidence, missing_fields, overall assessment.
    Migration creates processing_reports table with unique email_id constraint.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create processing report generation service</name>
  <files>
    app/services/processing_reports.py
  </files>
  <action>
    Create app/services/processing_reports.py:

    1. create_processing_report(
         db: Session,
         email_id: int,
         extracted_data: dict,
         agent_checkpoints: dict,
         overall_confidence: float,
         confidence_route: str,
         needs_review: bool,
         review_reason: str = None,
         processing_time_ms: int = None
       ) -> ProcessingReport:

       a. Build extracted_fields from extracted_data:
          - For each key field (client_name, creditor_name, debt_amount, reference_numbers):
            - If present and not None/empty: add with value, confidence, source
            - Confidence from agent_checkpoints or default 0.5
            - Source from extraction_metadata or "unknown"

       b. Build missing_fields:
          - List of required fields that are None or empty

       c. Get pipeline metadata from agent_checkpoints:
          - intent from agent_1_intent
          - sources_processed from agent_2_extraction
          - total_tokens_used from agent_2_extraction

       d. Create and add ProcessingReport
       e. db.flush() (not commit - caller controls transaction)
       f. Return report

    2. get_processing_report(db: Session, email_id: int) -> Optional[ProcessingReport]:
       - Simple query by email_id

    3. get_reports_by_date_range(db: Session, start_date: date, end_date: date) -> List[ProcessingReport]:
       - Query reports by created_at range
       - Order by created_at desc

    4. get_reports_needing_review(db: Session, limit: int = 100) -> List[ProcessingReport]:
       - Query where needs_review=True
       - Order by created_at desc

    All functions should handle gracefully if ProcessingReport already exists for email_id (update instead of insert).
  </action>
  <verify>
    python -c "from app.services.processing_reports import create_processing_report, get_processing_report; print('Processing reports service imports successfully')"
  </verify>
  <done>
    create_processing_report generates detailed per-email report from extraction results.
    Reports include: extracted_fields with confidence, missing_fields, pipeline metadata.
    Query functions support date range and needs_review filtering.
  </done>
</task>

</tasks>

<verification>
1. Sentry test: `python -c "from app.services.monitoring.error_tracking import init_sentry, set_processing_context; init_sentry(); print('Sentry disabled (no DSN) - expected in dev')"`
2. Model test: `python -c "from app.models.processing_report import ProcessingReport; print('Model imports')"`
3. Service test: `python -c "from app.services.processing_reports import create_processing_report; print('Service imports')"`
</verification>

<success_criteria>
- sentry-sdk[fastapi] added to requirements.txt
- init_sentry() initializes Sentry with FastAPI integration
- set_processing_context() provides email_id, actor, correlation_id to Sentry (REQ-OPS-04)
- ProcessingReport model stores per-email extraction details (REQ-OPS-06)
- create_processing_report() builds report from extraction results
- Reports show: what extracted (with confidence per field), what missing
</success_criteria>

<output>
After completion, create `.planning/phases/09-production-hardening-monitoring/09-04-SUMMARY.md`
</output>
