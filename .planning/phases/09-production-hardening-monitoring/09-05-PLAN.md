---
phase: 09-production-hardening-monitoring
plan: 05
type: execute
wave: 4
depends_on: ["09-01", "09-02", "09-03", "09-04"]
files_modified:
  - app/main.py
  - app/actors/email_processor.py
  - app/routers/webhook.py
  - app/services/entity_extractor_claude.py
  - app/services/mongodb_client.py
  - app/services/storage/gcs_client.py
autonomous: true

must_haves:
  truths:
    - "Correlation ID propagates from webhook through final database write"
    - "Circuit breakers wrap Claude API, MongoDB, and GCS calls"
    - "Metrics recorded for processing time, token usage, errors, confidence"
    - "Sentry context set at actor start, processing report created at completion"
    - "Auto-match notifications sent via existing email_notifier (REQ-OPS-05 preserved)"
  artifacts:
    - path: "app/actors/email_processor.py"
      provides: "Full monitoring integration"
      contains: "MetricsCollector"
    - path: "app/main.py"
      provides: "Sentry initialization at startup"
      contains: "init_sentry"
  key_links:
    - from: "app/routers/webhook.py"
      to: "app/actors/email_processor.py"
      via: "correlation_id parameter"
      pattern: "process_email.send.*correlation_id"
    - from: "app/actors/email_processor.py"
      to: "app/services/monitoring/circuit_breakers.py"
      via: "breaker calls"
      pattern: "get_claude_breaker|with_circuit_breaker"
    - from: "app/actors/email_processor.py"
      to: "app/services/processing_reports.py"
      via: "report creation"
      pattern: "create_processing_report"
---

<objective>
Integrate all monitoring infrastructure into the email processing pipeline: correlation ID propagation, circuit breakers on external services, metrics collection, Sentry context, and processing report generation.

Purpose: Wire together all Phase 9 components so the email processing pipeline has full observability and resilience.

Output: Complete monitoring integration with correlation IDs flowing through the entire pipeline, circuit breakers protecting external services, metrics recorded at key points, and processing reports generated for each email.
</objective>

<execution_context>
@/Users/luka.s/.claude/get-shit-done/workflows/execute-plan.md
@/Users/luka.s/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-production-hardening-monitoring/09-CONTEXT.md
@.planning/phases/09-production-hardening-monitoring/09-RESEARCH.md

# Prior plan summaries (dependencies)
@.planning/phases/09-production-hardening-monitoring/09-01-SUMMARY.md
@.planning/phases/09-production-hardening-monitoring/09-02-SUMMARY.md
@.planning/phases/09-production-hardening-monitoring/09-03-SUMMARY.md
@.planning/phases/09-production-hardening-monitoring/09-04-SUMMARY.md

# Files to modify
@app/main.py
@app/actors/email_processor.py
@app/routers/webhook.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Initialize monitoring at app startup and propagate correlation ID from webhook</name>
  <files>
    app/main.py
    app/routers/webhook.py
  </files>
  <action>
    1. Update app/main.py startup:

       a. Import monitoring components:
          - from app.services.monitoring.logging import setup_logging
          - from app.services.monitoring.error_tracking import init_sentry

       b. At module level (before app creation):
          - Call setup_logging() to configure JSON logging

       c. In startup_event():
          - Call init_sentry() to initialize Sentry (if DSN configured)
          - Log "monitoring_initialized" with sentry_enabled boolean

       d. Remove old structlog.configure() block (replaced by setup_logging)

    2. Update app/routers/webhook.py:

       a. Import: from asgi_correlation_id.context import correlation_id

       b. In webhook endpoint, after validation and before enqueueing:
          - Capture current correlation_id: current_correlation_id = correlation_id.get()
          - Pass to actor: process_email.send(email_id=email.id, correlation_id=current_correlation_id)

       This ensures the correlation ID from the HTTP request propagates to the Dramatiq actor.
  </action>
  <verify>
    python -c "from app.main import app; from app.routers.webhook import router; print('App and webhook import successfully')"
  </verify>
  <done>
    setup_logging() called at module level in main.py.
    init_sentry() called in startup_event.
    Webhook passes correlation_id to process_email actor.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add circuit breakers to external service calls</name>
  <files>
    app/services/entity_extractor_claude.py
    app/services/mongodb_client.py
    app/services/storage/gcs_client.py
  </files>
  <action>
    1. Update app/services/entity_extractor_claude.py:

       a. Import: from app.services.monitoring.circuit_breakers import get_claude_breaker, CircuitBreakerError

       b. In extract_entities() method, wrap the Claude API call:
          ```python
          breaker = get_claude_breaker()
          try:
              response = breaker.call(
                  self.client.messages.create,
                  model=self.model,
                  max_tokens=self.max_tokens,
                  messages=[{"role": "user", "content": prompt}]
              )
          except CircuitBreakerError:
              logger.error("claude_circuit_open", email_id=email_id)
              raise  # Let actor handle retry
          ```

       c. Similarly wrap any other Claude API calls in the file

    2. Update app/services/mongodb_client.py:

       a. Import: from app.services.monitoring.circuit_breakers import get_mongodb_breaker, CircuitBreakerError

       b. Wrap MongoDB operations (find, update, insert) with circuit breaker:
          - In update_creditor_debt() or similar write methods
          - In find methods used for matching

       c. Pattern:
          ```python
          breaker = get_mongodb_breaker()
          result = breaker.call(self.collection.update_one, filter, update)
          ```

    3. Update app/services/storage/gcs_client.py:

       a. Import: from app.services.monitoring.circuit_breakers import get_gcs_breaker, CircuitBreakerError

       b. Wrap GCS operations (download_blob, upload_blob) with circuit breaker:
          ```python
          breaker = get_gcs_breaker()
          content = breaker.call(blob.download_as_bytes)
          ```

    For all files: Handle CircuitBreakerError by logging and re-raising (actor will retry after circuit reset).
  </action>
  <verify>
    python -c "from app.services.entity_extractor_claude import entity_extractor_claude; from app.services.mongodb_client import mongodb_service; print('Services with circuit breakers import successfully')"
  </verify>
  <done>
    Claude API calls wrapped with claude_breaker.
    MongoDB operations wrapped with mongodb_breaker.
    GCS operations wrapped with gcs_breaker.
    CircuitBreakerError handled with logging and re-raise.
  </done>
</task>

<task type="auto">
  <name>Task 3: Integrate metrics, Sentry context, and processing reports into email processor</name>
  <files>
    app/actors/email_processor.py
  </files>
  <action>
    Update app/actors/email_processor.py with comprehensive monitoring:

    1. Add imports at top:
       ```python
       import time
       from app.services.monitoring.metrics import MetricsCollector
       from app.services.monitoring.error_tracking import set_processing_context, add_breadcrumb
       from app.services.processing_reports import create_processing_report
       from asgi_correlation_id.context import correlation_id as correlation_id_ctx
       ```

    2. At START of process_email() (after db session creation):

       a. Record start time: start_time = time.time()

       b. Restore correlation ID context:
          ```python
          if correlation_id:
              correlation_id_ctx.set(correlation_id)
          ```

       c. Set Sentry context:
          ```python
          set_processing_context(
              email_id=email_id,
              actor="process_email",
              correlation_id=correlation_id or "none"
          )
          ```

       d. Initialize metrics collector:
          ```python
          metrics = MetricsCollector(db)
          ```

    3. Add breadcrumbs at key stages:
       - After intent classification: add_breadcrumb("pipeline", f"Intent: {intent}", data={"intent": intent_result})
       - After extraction: add_breadcrumb("pipeline", f"Extracted amount: {amount}", data={"sources": sources_count})
       - After matching: add_breadcrumb("pipeline", f"Match status: {status}", data={"score": score})

    4. Record metrics at key points:

       a. After extraction completes:
          ```python
          metrics.record_token_usage(
              model="claude-sonnet",
              operation="extraction",
              tokens=extraction_result.get("total_tokens_used", 0),
              email_id=email_id
          )
          ```

       b. After confidence calculation:
          ```python
          confidence_bucket = "high" if overall >= 0.85 else "medium" if overall >= 0.6 else "low"
          metrics.record_confidence(confidence_bucket, overall, email_id=email_id)
          ```

    5. In exception handler:
       ```python
       metrics.record_error("process_email", type(e).__name__, email_id=email_id)
       ```

    6. In finally block (after gc.collect()):

       a. Record processing time:
          ```python
          duration_ms = int((time.time() - start_time) * 1000)
          metrics.record_processing_time("process_email", "complete", duration_ms, email_id=email_id)
          ```

    7. Before final commit (on success path):

       a. Create processing report:
          ```python
          try:
              create_processing_report(
                  db=db,
                  email_id=email_id,
                  extracted_data=email.extracted_data,
                  agent_checkpoints=email.agent_checkpoints or {},
                  overall_confidence=confidence_result.overall if confidence_result else 0.5,
                  confidence_route=email.confidence_route or "unknown",
                  needs_review=bool(email.match_status == "needs_review"),
                  review_reason=consolidation_result.get("review_reason") if consolidation_result else None,
                  processing_time_ms=duration_ms
              )
          except Exception as report_error:
              logger.warning("processing_report_failed", error=str(report_error), email_id=email_id)
              # Don't fail processing for report generation errors
          ```

    8. Verify auto-match notification (REQ-OPS-05):
       - Confirm email_notifier.send_debt_update_notification() is still called in UPDATE_AND_NOTIFY path
       - This is existing v1 functionality that must be preserved
       - Add breadcrumb: add_breadcrumb("notification", "Auto-match notification sent")
  </action>
  <verify>
    python -c "from app.actors.email_processor import process_email; print('Email processor with full monitoring imports successfully')"
  </verify>
  <done>
    Correlation ID restored at actor start.
    Sentry context set with email_id, actor, correlation_id.
    Breadcrumbs added at key pipeline stages.
    Metrics recorded: token_usage, confidence, processing_time, errors.
    Processing report created on successful completion.
    Auto-match notification preserved (REQ-OPS-05).
  </done>
</task>

</tasks>

<verification>
1. App startup test: `python -c "from app.main import app; print('App with monitoring startup imports')"`
2. Correlation flow test: Start app, make webhook request, check logs for correlation_id in actor logs
3. Circuit breaker test: Manually trigger 5 failures, verify circuit opens and email alert sent (if SMTP configured)
4. Metrics test: Process an email, query operational_metrics table for records
5. Processing report test: Process an email, query processing_reports table for report
</verification>

<success_criteria>
- Correlation ID flows: webhook -> actor parameter -> Sentry context -> all logs
- Circuit breakers wrap: Claude API, MongoDB, GCS (REQ-OPS-03)
- Metrics recorded: processing_time, token_usage, errors, confidence (REQ-OPS-02)
- Sentry context includes email_id, actor, correlation_id (REQ-OPS-04)
- Processing report created per email with extraction details (REQ-OPS-06)
- Auto-match notifications preserved (REQ-OPS-05)
- REQ-OPS-01 satisfied: Correlation ID propagates webhook through final write
</success_criteria>

<output>
After completion, create `.planning/phases/09-production-hardening-monitoring/09-05-SUMMARY.md`
</output>
