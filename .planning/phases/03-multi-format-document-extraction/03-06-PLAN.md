---
phase: 03-multi-format-document-extraction
plan: 06
type: execute
wave: 4
depends_on: ["03-02", "03-03", "03-04", "03-05"]
files_modified:
  - app/actors/content_extractor.py
  - app/actors/__init__.py
  - app/actors/email_processor.py
autonomous: true

must_haves:
  truths:
    - "Content extraction runs as Dramatiq actor"
    - "Actor downloads attachments, extracts from all sources, consolidates"
    - "Daily circuit breaker halts processing when cost limit exceeded"
    - "Email processor delegates extraction to content extractor"
    - "Extraction results stored in IncomingEmail.extracted_data"
  artifacts:
    - path: "app/actors/content_extractor.py"
      provides: "Dramatiq actor for content extraction"
      contains: "def extract_content"
    - path: "app/actors/email_processor.py"
      provides: "Updated to call content extractor"
      contains: "extract_content"
  key_links:
    - from: "app/actors/content_extractor.py"
      to: "app/services/extraction"
      via: "orchestrates all extractors"
      pattern: "ExtractionConsolidator"
    - from: "app/actors/content_extractor.py"
      to: "app/services/storage/gcs_client.py"
      via: "downloads attachments"
      pattern: "GCSAttachmentHandler"
    - from: "app/actors/content_extractor.py"
      to: "app/services/cost_control/circuit_breaker.py"
      via: "daily cost limit check"
      pattern: "DailyCostCircuitBreaker"
---

<objective>
Create Dramatiq actor that orchestrates the extraction pipeline and integrate with email processor.

Purpose: Ties together all extraction components into an async job that processes email body + attachments and produces consolidated extraction results.

Output: Content extractor Dramatiq actor, updated email processor that delegates extraction.
</objective>

<execution_context>
@/Users/luka.s/.claude/get-shit-done/workflows/execute-plan.md
@/Users/luka.s/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-multi-format-document-extraction/03-CONTEXT.md
@.planning/phases/03-multi-format-document-extraction/03-RESEARCH.md

# Phase 2 infrastructure
@.planning/phases/02-async-job-queue-infrastructure/02-03-SUMMARY.md

# Current email processor
@app/actors/email_processor.py
@app/actors/__init__.py

# All extraction services
@app/services/extraction/__init__.py
@app/services/storage/gcs_client.py
@app/services/cost_control/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create content extractor Dramatiq actor</name>
  <files>
app/actors/content_extractor.py
app/actors/__init__.py
  </files>
  <action>
Create Dramatiq actor that orchestrates the extraction pipeline.

**File: app/actors/content_extractor.py**

```python
"""
Content Extractor Actor

Orchestrates extraction from email body + all attachments:
1. Downloads attachments from GCS/URLs
2. Routes each to appropriate extractor based on format
3. Consolidates results using business rules
4. Returns ConsolidatedExtractionResult
"""

import dramatiq
import structlog
import gc
import os
from typing import List, Optional, Dict, Any

from app.actors import broker
from app.config import settings
from app.models.extraction_result import (
    SourceExtractionResult,
    ConsolidatedExtractionResult,
)
from app.services.extraction import (
    detect_file_format,
    FileFormat,
    EmailBodyExtractor,
    PDFExtractor,
    DOCXExtractor,
    XLSXExtractor,
    ImageExtractor,
    ExtractionConsolidator,
)
from app.services.storage import GCSAttachmentHandler, FileTooLargeError
from app.services.cost_control import (
    TokenBudgetTracker,
    TokenBudgetExceeded,
    DailyCostCircuitBreaker,
    DailyLimitExceeded,
)

logger = structlog.get_logger()


class ContentExtractionService:
    """
    Orchestrates extraction from all sources for an email.

    Handles:
    - Email body text extraction
    - Attachment download and format routing
    - Token budget enforcement
    - Daily cost circuit breaker
    - Result consolidation
    """

    def __init__(self, redis_client=None):
        self.token_budget = TokenBudgetTracker()

        # Initialize circuit breaker if Redis available
        if redis_client:
            self.circuit_breaker = DailyCostCircuitBreaker(redis_client)
        else:
            self.circuit_breaker = None

        # Initialize extractors
        self.email_extractor = EmailBodyExtractor()
        self.pdf_extractor = PDFExtractor(token_budget=self.token_budget)
        self.docx_extractor = DOCXExtractor()
        self.xlsx_extractor = XLSXExtractor()
        self.image_extractor = ImageExtractor(token_budget=self.token_budget)
        self.consolidator = ExtractionConsolidator()
        self.gcs_handler = GCSAttachmentHandler()

    def extract_all(
        self,
        email_body: Optional[str],
        attachment_urls: Optional[List[Dict[str, Any]]]
    ) -> ConsolidatedExtractionResult:
        """
        Extract from email body and all attachments.

        Args:
            email_body: Cleaned email body text
            attachment_urls: List of attachment metadata dicts with url, filename, content_type, size

        Returns:
            ConsolidatedExtractionResult with merged data
        """
        source_results: List[SourceExtractionResult] = []

        # Check circuit breaker first
        if self.circuit_breaker and self.circuit_breaker.is_open():
            logger.warning("daily_cost_limit_exceeded", action="skipping_extraction")
            return self._make_circuit_breaker_result()

        # 1. Extract from email body
        if email_body:
            try:
                body_result = self.email_extractor.extract(email_body)
                source_results.append(body_result)
                logger.info("email_body_extracted",
                    has_amount=body_result.gesamtforderung is not None)
            except Exception as e:
                logger.error("email_body_extraction_failed", error=str(e))

        # 2. Process attachments in priority order (PDFs first, then DOCX/XLSX, then images)
        if attachment_urls:
            # Sort by priority: PDF > DOCX > XLSX > Images
            priority_order = {
                FileFormat.PDF: 1,
                FileFormat.DOCX: 2,
                FileFormat.XLSX: 3,
                FileFormat.IMAGE_JPG: 4,
                FileFormat.IMAGE_PNG: 4,
            }

            sorted_attachments = sorted(
                attachment_urls,
                key=lambda a: priority_order.get(
                    detect_file_format(a.get("filename", ""), a.get("content_type")),
                    5
                )
            )

            for attachment in sorted_attachments:
                # Check if we still have token budget
                if self.token_budget.remaining() < 1000:
                    logger.warning("token_budget_low", remaining=self.token_budget.remaining())
                    break

                result = self._extract_attachment(attachment)
                if result:
                    source_results.append(result)

        # 3. Consolidate all results
        consolidated = self.consolidator.consolidate(source_results)

        # 4. Record cost to circuit breaker
        if self.circuit_breaker and consolidated.total_tokens_used > 0:
            cost_usd = self.token_budget.estimate_cost_usd()
            self.circuit_breaker.check_and_record(cost_usd)

        logger.info("extraction_complete",
            sources=len(source_results),
            sources_with_amount=consolidated.sources_with_amount,
            final_amount=consolidated.gesamtforderung,
            confidence=consolidated.confidence,
            tokens_used=consolidated.total_tokens_used)

        # Memory cleanup
        gc.collect()

        return consolidated

    def _extract_attachment(self, attachment: Dict[str, Any]) -> Optional[SourceExtractionResult]:
        """Extract from a single attachment."""
        url = attachment.get("url")
        filename = attachment.get("filename", "unknown")
        content_type = attachment.get("content_type")

        if not url:
            logger.warning("attachment_missing_url", filename=filename)
            return None

        file_format = detect_file_format(filename, content_type)

        if file_format == FileFormat.UNKNOWN:
            logger.info("unsupported_format", filename=filename, content_type=content_type)
            return None

        try:
            # Download attachment to temp file
            with self.gcs_handler.download_from_url(url) as temp_path:
                return self._extract_from_file(temp_path, file_format, filename)

        except FileTooLargeError as e:
            logger.warning("attachment_too_large", filename=filename, error=str(e))
            return SourceExtractionResult(
                source_type=file_format.value,
                source_name=filename,
                extraction_method="skipped",
                error="file_too_large"
            )
        except Exception as e:
            logger.error("attachment_download_failed", filename=filename, error=str(e))
            return SourceExtractionResult(
                source_type=file_format.value if file_format != FileFormat.UNKNOWN else "unknown",
                source_name=filename,
                extraction_method="failed",
                error=str(e)
            )

    def _extract_from_file(
        self,
        file_path: str,
        file_format: FileFormat,
        filename: str
    ) -> SourceExtractionResult:
        """Route to appropriate extractor based on format."""
        try:
            if file_format == FileFormat.PDF:
                return self.pdf_extractor.extract(file_path)
            elif file_format == FileFormat.DOCX:
                return self.docx_extractor.extract(file_path)
            elif file_format == FileFormat.XLSX:
                return self.xlsx_extractor.extract(file_path)
            elif file_format in (FileFormat.IMAGE_JPG, FileFormat.IMAGE_PNG):
                return self.image_extractor.extract(file_path)
            else:
                return SourceExtractionResult(
                    source_type="unknown",
                    source_name=filename,
                    extraction_method="skipped",
                    error=f"unsupported_format: {file_format}"
                )
        except TokenBudgetExceeded as e:
            logger.warning("token_budget_exceeded", filename=filename)
            return SourceExtractionResult(
                source_type=file_format.value,
                source_name=filename,
                extraction_method="skipped",
                error=str(e)
            )
        except Exception as e:
            logger.error("extraction_failed", filename=filename, error=str(e))
            return SourceExtractionResult(
                source_type=file_format.value,
                source_name=filename,
                extraction_method="failed",
                error=str(e)
            )

    def _make_circuit_breaker_result(self) -> ConsolidatedExtractionResult:
        """Return result when circuit breaker is open."""
        return ConsolidatedExtractionResult(
            gesamtforderung=100.0,  # Default amount
            client_name=None,
            creditor_name=None,
            confidence="LOW",
            sources_processed=0,
            sources_with_amount=0,
            total_tokens_used=0,
            source_results=[]
        )


# Dramatiq actor for async extraction
@dramatiq.actor(
    broker=broker,
    max_retries=3,
    min_backoff=15000,  # 15 seconds
    max_backoff=300000,  # 5 minutes
)
def extract_content(
    email_id: int,
    email_body: Optional[str],
    attachment_urls: Optional[List[Dict[str, Any]]]
) -> Dict[str, Any]:
    """
    Dramatiq actor for content extraction.

    Called by email_processor actor after initial validation.

    Args:
        email_id: IncomingEmail.id for logging
        email_body: Cleaned email body text
        attachment_urls: List of attachment metadata

    Returns:
        Dict representation of ConsolidatedExtractionResult
    """
    logger.info("extract_content_started", email_id=email_id)

    # Get Redis client from broker if available
    redis_client = None
    if settings.redis_url:
        import redis
        redis_client = redis.from_url(settings.redis_url)

    service = ContentExtractionService(redis_client=redis_client)

    try:
        result = service.extract_all(email_body, attachment_urls)

        logger.info("extract_content_completed",
            email_id=email_id,
            amount=result.gesamtforderung,
            confidence=result.confidence,
            tokens=result.total_tokens_used)

        # Return as dict for serialization
        return result.model_dump()

    except Exception as e:
        logger.error("extract_content_failed", email_id=email_id, error=str(e))
        raise
    finally:
        gc.collect()
```

**Update app/actors/__init__.py:**
Add import for content_extractor:
```python
# After broker setup, import actors to register them
from app.actors import email_processor
from app.actors import content_extractor  # Add this line
```

**Implementation notes:**
- Service class for testability (can instantiate without Dramatiq)
- Actor is thin wrapper around service
- Priority order: PDFs first (highest info density), images last (highest cost)
- Circuit breaker check before any extraction
- Memory cleanup after each extraction
  </action>
  <verify>
```bash
cd "/Users/luka.s/NEW AI Creditor Answer Analysis"
python -c "
from app.actors.content_extractor import ContentExtractionService, extract_content
print('ContentExtractionService import OK')
print('extract_content actor import OK')

# Test service instantiation (no Redis)
service = ContentExtractionService()
print('Service instantiated without Redis OK')
"
```
  </verify>
  <done>ContentExtractionService orchestrates all extractors, extract_content Dramatiq actor wraps service for async execution</done>
</task>

<task type="auto">
  <name>Task 2: Integrate content extractor with email processor</name>
  <files>app/actors/email_processor.py</files>
  <action>
Update email processor actor to call content extractor and store results.

**Changes to app/actors/email_processor.py:**

1. Import content extraction:
```python
from app.actors.content_extractor import ContentExtractionService
from app.models.extraction_result import ConsolidatedExtractionResult
```

2. In the `_process_email` method (or equivalent main processing function), replace or augment the existing entity extraction with:

```python
def _process_email(self, email: IncomingEmail, db: Session) -> None:
    """Process a single email - extract content and update database."""

    # ... existing validation code ...

    # NEW: Use content extraction pipeline
    extraction_service = ContentExtractionService(redis_client=self._get_redis_client())

    # Get email body (use cleaned_body if available, else raw)
    email_body = email.cleaned_body or email.raw_body_text or email.raw_body_html

    # Get attachment URLs from JSON column
    attachment_urls = email.attachment_urls or []

    # Run extraction
    result = extraction_service.extract_all(
        email_body=email_body,
        attachment_urls=attachment_urls
    )

    # Store result in extracted_data JSON column
    email.extracted_data = {
        "is_creditor_reply": True,  # Determined by intent classification in Phase 5
        "client_name": result.client_name,
        "creditor_name": result.creditor_name,
        "debt_amount": result.gesamtforderung,
        "reference_numbers": [],  # TODO: Extract in Phase 4
        "confidence": self._confidence_to_float(result.confidence),
        "extraction_metadata": {
            "sources_processed": result.sources_processed,
            "sources_with_amount": result.sources_with_amount,
            "total_tokens_used": result.total_tokens_used,
            "method": "phase3_extraction"
        }
    }

    # Update processing status
    email.processing_status = "completed"
    email.processed_at = datetime.utcnow()
    email.completed_at = datetime.utcnow()

    db.commit()

    logger.info("email_processed",
        email_id=email.id,
        amount=result.gesamtforderung,
        confidence=result.confidence)

def _confidence_to_float(self, confidence: str) -> float:
    """Convert confidence level to float for backward compatibility."""
    mapping = {"HIGH": 0.9, "MEDIUM": 0.7, "LOW": 0.5}
    return mapping.get(confidence, 0.5)

def _get_redis_client(self):
    """Get Redis client if available."""
    if settings.redis_url:
        import redis
        return redis.from_url(settings.redis_url)
    return None
```

**Key changes:**
1. Replace existing EntityExtractor call with ContentExtractionService
2. Use email.attachment_urls (populated by webhook in Phase 2)
3. Store consolidated result in extracted_data JSON
4. Maintain backward-compatible schema (is_creditor_reply, client_name, creditor_name, debt_amount, confidence)
5. Add extraction_metadata for debugging

**Preserve backward compatibility:**
- extracted_data schema remains the same
- confidence converted to float (0-1 range)
- is_creditor_reply left as True (Phase 5 will add intent classification)

**Implementation notes:**
- Don't remove existing EntityExtractor code yet - Phase 4/5 may need it
- Log extraction results at info level
- Handle case where attachment_urls is None
  </action>
  <verify>
```bash
cd "/Users/luka.s/NEW AI Creditor Answer Analysis"
python -c "
from app.actors.email_processor import process_email
print('email_processor with content extraction integration OK')
"
# Check imports work
python -c "
from app.actors.content_extractor import ContentExtractionService
from app.models.extraction_result import ConsolidatedExtractionResult
print('All imports for integration OK')
"
```
  </verify>
  <done>Email processor calls ContentExtractionService, stores consolidated results in extracted_data, maintains backward-compatible schema</done>
</task>

</tasks>

<verification>
After both tasks complete:
1. ContentExtractionService orchestrates all extractors
2. extract_content Dramatiq actor registered with broker
3. Email processor delegates to content extraction
4. extracted_data contains consolidated results
5. Memory cleanup (gc.collect) after extraction
</verification>

<success_criteria>
- Content extraction runs as Dramatiq actor
- All sources (email body + attachments) processed
- Daily circuit breaker checked before extraction
- Results stored in IncomingEmail.extracted_data
- Backward-compatible extracted_data schema
- Tokens and sources tracked in metadata
</success_criteria>

<output>
After completion, create `.planning/phases/03-multi-format-document-extraction/03-06-SUMMARY.md`
</output>
