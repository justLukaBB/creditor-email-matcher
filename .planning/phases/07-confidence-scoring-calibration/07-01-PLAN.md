---
phase: 07-confidence-scoring-calibration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - app/services/confidence/__init__.py
  - app/services/confidence/dimensions.py
  - app/models/calibration_sample.py
  - app/models/__init__.py
  - alembic/versions/20260205_2300_add_calibration_samples.py
autonomous: true

must_haves:
  truths:
    - "extraction_confidence is calculated from document type and extraction completeness"
    - "match_confidence is derived from matching engine score"
    - "Calibration samples can be stored in database"
  artifacts:
    - path: "app/services/confidence/dimensions.py"
      provides: "Confidence dimension calculators"
      exports: ["calculate_extraction_confidence", "calculate_match_confidence"]
    - path: "app/models/calibration_sample.py"
      provides: "CalibrationSample model for storing labeled examples"
      contains: "class CalibrationSample"
    - path: "alembic/versions/20260205_2300_add_calibration_samples.py"
      provides: "Migration for calibration_samples table"
      contains: "def upgrade"
  key_links:
    - from: "app/services/confidence/dimensions.py"
      to: "agent_checkpoints JSONB"
      via: "extraction source quality lookup"
      pattern: "agent_2_extraction|source_type"
    - from: "app/services/confidence/dimensions.py"
      to: "match_result score"
      via: "matching confidence passthrough"
      pattern: "total_score|match_confidence"
---

<objective>
Create confidence dimension calculators and calibration data model

Purpose: Establish separate confidence dimensions (extraction_confidence, match_confidence) that can be combined into overall confidence, and create the database infrastructure for storing calibration samples from reviewer corrections.

Output:
- Confidence dimension service with extraction and match confidence calculators
- CalibrationSample model with migration for storing labeled examples
</objective>

<execution_context>
@/Users/luka.s/.claude/get-shit-done/workflows/execute-plan.md
@/Users/luka.s/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-confidence-scoring-calibration/07-CONTEXT.md
@app/models/incoming_email.py
@app/services/validation/confidence_checker.py
@app/models/match_result.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create confidence dimensions service</name>
  <files>
    app/services/confidence/__init__.py
    app/services/confidence/dimensions.py
  </files>
  <action>
Create confidence dimension calculators that compute separate confidence scores for extraction and matching stages.

1. Create `app/services/confidence/__init__.py`:
   - Export calculate_extraction_confidence, calculate_match_confidence

2. Create `app/services/confidence/dimensions.py`:

**calculate_extraction_confidence(agent_checkpoints: dict, document_types: list[str]) -> float:**
- Takes agent_checkpoints JSONB and list of processed document types
- Implements USER DECISION: document-level only (not field-level)
- Combines source quality baseline + completeness adjustment:
  - Source quality baselines (Claude's discretion per CONTEXT.md):
    - native_pdf: 0.95 (text extraction, no OCR needed)
    - scanned_pdf: 0.75 (Claude Vision, OCR variability)
    - docx: 0.90 (structured text)
    - xlsx: 0.85 (tabular, may need context)
    - image: 0.70 (Claude Vision, least reliable)
    - email_body: 0.80 (text but often noisy)
  - Use minimum source quality across all processed sources (weakest-link at source level)
  - Completeness adjustment: if key fields missing (amount, client_name, creditor_name), reduce by 0.1 per missing field
  - Floor at 0.3 (never return confidence below 0.3)
  - Ceiling at 1.0
- Return float 0.0-1.0

**calculate_match_confidence(match_result: Optional[dict]) -> float:**
- Takes match result from MatchingEngineV2
- If no match_result or status in (no_candidates, no_recent_inquiry): return 0.0
- If status is auto_matched: return total_score directly (already 0.0-1.0)
- If status is ambiguous: factor in gap (Claude's discretion per CONTEXT.md):
  - Use score * (1 - 0.3) when ambiguous (reduce confidence due to uncertainty)
- If status is below_threshold: return total_score (already low)
- Return float 0.0-1.0

Use structlog for logging confidence calculations with full context.
  </action>
  <verify>
python -c "from app.services.confidence import calculate_extraction_confidence, calculate_match_confidence; print('imports ok')"
  </verify>
  <done>
- calculate_extraction_confidence returns float based on source quality baselines and completeness
- calculate_match_confidence returns float from matching engine score with ambiguity adjustment
- Both functions handle edge cases (empty inputs, missing fields)
  </done>
</task>

<task type="auto">
  <name>Task 2: Create CalibrationSample model and migration</name>
  <files>
    app/models/calibration_sample.py
    app/models/__init__.py
    alembic/versions/20260205_2300_add_calibration_samples.py
  </files>
  <action>
Create database model for storing calibration samples that will be used to tune confidence thresholds over time.

1. Create `app/models/calibration_sample.py`:

```python
"""
CalibrationSample Model
Stores labeled examples from reviewer corrections for confidence threshold calibration
"""

from sqlalchemy import Column, Integer, String, DateTime, Float, Boolean, ForeignKey, Text
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.sql import func
from app.database import Base


class CalibrationSample(Base):
    """
    Represents a calibration sample captured from manual review resolution.

    Labels are captured implicitly:
    - If reviewer changes data, original extraction was WRONG (label=False)
    - If reviewer approves without changes, original extraction was CORRECT (label=True)

    Used for:
    - Threshold auto-adjustment (accumulate 500+ samples per category)
    - Performance monitoring (track accuracy over time)
    - Document type analysis (native PDF vs scanned vs image)
    """
    __tablename__ = "calibration_samples"

    # Primary Key
    id = Column(Integer, primary_key=True, index=True)

    # Reference to source
    email_id = Column(Integer, ForeignKey("incoming_emails.id"), nullable=False, index=True)
    review_id = Column(Integer, ForeignKey("manual_review_queue.id"), nullable=True, index=True)

    # What we predicted
    predicted_confidence = Column(Float, nullable=False)  # Overall confidence at processing time
    extraction_confidence = Column(Float, nullable=True)  # Extraction dimension
    match_confidence = Column(Float, nullable=True)  # Match dimension
    document_type = Column(String(50), nullable=True)  # native_pdf, scanned_pdf, image, etc.

    # Ground truth label
    was_correct = Column(Boolean, nullable=False)  # True if approved without changes

    # What was changed (for analysis)
    correction_type = Column(String(50), nullable=True)
    # Types: amount_corrected, name_corrected, creditor_corrected, match_corrected, multiple

    correction_details = Column(JSONB, nullable=True)
    """
    Details of what was corrected for analysis.

    Structure:
    {
        "original_amount": 1500.0,
        "corrected_amount": 1550.0,
        "original_client": "Muller Max",
        "corrected_client": "Mueller Max",
        "field_changes": ["amount", "client_name"]
    }
    """

    # Categorization for threshold tuning
    confidence_bucket = Column(String(20), nullable=False)
    # Buckets: high (>0.85), medium (0.6-0.85), low (<0.6)

    # Timestamps
    captured_at = Column(DateTime(timezone=True), server_default=func.now(), nullable=False)

    def __repr__(self):
        return f"<CalibrationSample(id={self.id}, email_id={self.email_id}, was_correct={self.was_correct}, bucket='{self.confidence_bucket}')>"
```

2. Update `app/models/__init__.py`:
   - Add import for CalibrationSample
   - Add to __all__ list

3. Create migration `alembic/versions/20260205_2300_add_calibration_samples.py`:
   - Create calibration_samples table with all columns
   - Add indexes on email_id, confidence_bucket, captured_at
   - Add index for bucket+correct analysis: (confidence_bucket, was_correct)

Note: Use existing migration pattern from Phase 5/6 migrations as reference.
  </action>
  <verify>
alembic upgrade head && python -c "from app.models import CalibrationSample; print('model ok')"
  </verify>
  <done>
- CalibrationSample model exists with all fields for storing labeled examples
- Migration creates calibration_samples table with appropriate indexes
- Model exported from app.models
  </done>
</task>

</tasks>

<verification>
After both tasks complete:
1. `python -c "from app.services.confidence import calculate_extraction_confidence, calculate_match_confidence; print('confidence service ok')"`
2. `python -c "from app.models import CalibrationSample; print('model ok')"`
3. `alembic current` shows latest migration applied
4. Check table exists: `psql $DATABASE_URL -c '\d calibration_samples'` (if DB available)
</verification>

<success_criteria>
- Confidence dimension calculators return floats 0.0-1.0 based on documented logic
- Source quality baselines differentiate native PDF (0.95) from scanned (0.75) from image (0.70)
- Match confidence factors in ambiguity status
- CalibrationSample model stores all fields needed for threshold tuning
- Migration creates table with appropriate indexes
</success_criteria>

<output>
After completion, create `.planning/phases/07-confidence-scoring-calibration/07-01-SUMMARY.md`
</output>
