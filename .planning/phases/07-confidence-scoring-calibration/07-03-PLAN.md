---
phase: 07-confidence-scoring-calibration
plan: 03
type: execute
wave: 2
depends_on: ["07-01"]
files_modified:
  - app/services/calibration/__init__.py
  - app/services/calibration/collector.py
  - app/routers/manual_review.py
  - app/services/validation/review_queue.py
autonomous: true

must_haves:
  truths:
    - "Reviewer corrections are captured as calibration samples"
    - "Approval without changes marks sample as correct (was_correct=True)"
    - "Any correction marks sample as incorrect (was_correct=False)"
    - "Calibration samples include confidence breakdown at time of processing"
  artifacts:
    - path: "app/services/calibration/collector.py"
      provides: "Calibration sample collection from review resolutions"
      exports: ["capture_calibration_sample"]
    - path: "app/routers/manual_review.py"
      provides: "Updated resolve endpoint that captures calibration data"
      contains: "capture_calibration_sample"
  key_links:
    - from: "app/routers/manual_review.py"
      to: "app/services/calibration/collector.py"
      via: "resolve endpoint calls collector"
      pattern: "capture_calibration_sample"
    - from: "app/services/calibration/collector.py"
      to: "app/models/calibration_sample.py"
      via: "creates CalibrationSample records"
      pattern: "CalibrationSample"
---

<objective>
Implement calibration data collection from manual review resolutions

Purpose: Capture implicit labels from reviewer actions - approvals without changes indicate correct predictions, corrections indicate errors. This data enables threshold auto-adjustment over time.

Output:
- Calibration collector service that captures samples from resolutions
- Updated manual review resolution endpoint that triggers collection
</objective>

<execution_context>
@/Users/luka.s/.claude/get-shit-done/workflows/execute-plan.md
@/Users/luka.s/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-confidence-scoring-calibration/07-CONTEXT.md
@app/models/calibration_sample.py
@app/models/manual_review.py
@app/routers/manual_review.py
@app/models/incoming_email.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create calibration collector service</name>
  <files>
    app/services/calibration/__init__.py
    app/services/calibration/collector.py
  </files>
  <action>
Create the calibration sample collector that captures labeled examples from manual review resolutions.

1. Create `app/services/calibration/__init__.py`:
```python
"""
Calibration Services
Collect and analyze calibration data for threshold tuning
"""

from app.services.calibration.collector import capture_calibration_sample

__all__ = ["capture_calibration_sample"]
```

2. Create `app/services/calibration/collector.py`:

```python
"""
Calibration Sample Collector
Captures labeled examples from manual review resolutions for threshold tuning

USER DECISIONS from CONTEXT.md:
- Labels captured implicitly from reviewer corrections
- If reviewer changes data, original was wrong (was_correct=False)
- If reviewer approves without changes, original was correct (was_correct=True)
"""

from typing import Dict, Any, Optional, List
from sqlalchemy.orm import Session
import structlog

from app.models.calibration_sample import CalibrationSample
from app.models.incoming_email import IncomingEmail
from app.models.manual_review import ManualReviewQueue
from app.config import settings

logger = structlog.get_logger(__name__)


def _determine_confidence_bucket(confidence: float) -> str:
    """
    Categorize confidence into bucket for analysis.

    Uses same thresholds as router for consistency.
    """
    if confidence >= settings.confidence_high_threshold:
        return "high"
    elif confidence >= settings.confidence_low_threshold:
        return "medium"
    else:
        return "low"


def _detect_correction_type(
    original_data: Dict[str, Any],
    corrected_data: Optional[Dict[str, Any]]
) -> tuple[str, Dict[str, Any]]:
    """
    Detect what type of correction was made and capture details.

    Returns:
        (correction_type, correction_details)
    """
    if not corrected_data:
        return ("none", {})

    field_changes = []
    details = {}

    # Check amount correction
    orig_amount = original_data.get("debt_amount")
    corr_amount = corrected_data.get("debt_amount")
    if orig_amount != corr_amount:
        field_changes.append("amount")
        details["original_amount"] = orig_amount
        details["corrected_amount"] = corr_amount

    # Check client name correction
    orig_client = original_data.get("client_name")
    corr_client = corrected_data.get("client_name")
    if orig_client != corr_client:
        field_changes.append("client_name")
        details["original_client"] = orig_client
        details["corrected_client"] = corr_client

    # Check creditor name correction
    orig_creditor = original_data.get("creditor_name")
    corr_creditor = corrected_data.get("creditor_name")
    if orig_creditor != corr_creditor:
        field_changes.append("creditor_name")
        details["original_creditor"] = orig_creditor
        details["corrected_creditor"] = corr_creditor

    # Check match correction (if different inquiry was selected)
    orig_inquiry = original_data.get("matched_inquiry_id")
    corr_inquiry = corrected_data.get("matched_inquiry_id")
    if orig_inquiry != corr_inquiry:
        field_changes.append("match")
        details["original_inquiry_id"] = orig_inquiry
        details["corrected_inquiry_id"] = corr_inquiry

    details["field_changes"] = field_changes

    # Determine correction type
    if len(field_changes) == 0:
        return ("none", details)
    elif len(field_changes) == 1:
        return (f"{field_changes[0]}_corrected", details)
    else:
        return ("multiple", details)


def _extract_document_types(agent_checkpoints: Optional[Dict[str, Any]]) -> str:
    """
    Extract primary document type from agent checkpoints.

    Returns the most significant document type processed.
    """
    if not agent_checkpoints:
        return "unknown"

    extraction_checkpoint = agent_checkpoints.get("agent_2_extraction", {})
    sources = extraction_checkpoint.get("sources_processed", [])

    # Priority: native_pdf > scanned_pdf > docx > xlsx > image > email_body
    priority = ["native_pdf", "scanned_pdf", "docx", "xlsx", "image", "email_body"]

    for doc_type in priority:
        if doc_type in sources or any(doc_type in str(s) for s in sources):
            return doc_type

    return "email_body"  # Default if no attachments


def capture_calibration_sample(
    db: Session,
    review_item: ManualReviewQueue,
    email: IncomingEmail,
    resolution: str,
    corrected_data: Optional[Dict[str, Any]] = None
) -> Optional[int]:
    """
    Capture a calibration sample from a manual review resolution.

    USER DECISION: Labels captured implicitly from reviewer corrections.

    Args:
        db: Database session
        review_item: The resolved ManualReviewQueue item
        email: The IncomingEmail being reviewed
        resolution: Resolution type (approved, corrected, rejected, etc.)
        corrected_data: If corrected, the new values

    Returns:
        CalibrationSample.id if created, None if skipped
    """
    # Skip spam/rejected - not useful for calibration
    if resolution in ("spam", "rejected", "escalated"):
        logger.info(
            "calibration_sample_skipped",
            email_id=email.id,
            resolution=resolution,
            reason="resolution_type_not_useful"
        )
        return None

    # Determine if prediction was correct
    # approved without changes = correct
    # corrected = incorrect
    was_correct = resolution == "approved"

    # Get original extracted data
    original_data = email.extracted_data or {}

    # Get confidence values from various sources
    predicted_confidence = original_data.get("confidence", 0.5)

    # Try to get dimension breakdown from pipeline_metadata
    pipeline_meta = original_data.get("pipeline_metadata", {})
    extraction_confidence = None  # Will be calculated in Phase 7 integration
    match_confidence = email.match_confidence / 100.0 if email.match_confidence else None

    # Detect correction type if corrected
    correction_type, correction_details = _detect_correction_type(
        original_data,
        corrected_data
    )

    # Get document type
    document_type = _extract_document_types(email.agent_checkpoints)

    # Determine confidence bucket
    confidence_bucket = _determine_confidence_bucket(predicted_confidence)

    # Create calibration sample
    sample = CalibrationSample(
        email_id=email.id,
        review_id=review_item.id,
        predicted_confidence=predicted_confidence,
        extraction_confidence=extraction_confidence,
        match_confidence=match_confidence,
        document_type=document_type,
        was_correct=was_correct,
        correction_type=correction_type if not was_correct else None,
        correction_details=correction_details if not was_correct else None,
        confidence_bucket=confidence_bucket
    )

    db.add(sample)
    db.flush()

    logger.info(
        "calibration_sample_captured",
        sample_id=sample.id,
        email_id=email.id,
        was_correct=was_correct,
        confidence_bucket=confidence_bucket,
        correction_type=correction_type,
        document_type=document_type
    )

    return sample.id
```
  </action>
  <verify>
python -c "from app.services.calibration import capture_calibration_sample; print('collector ok')"
  </verify>
  <done>
- capture_calibration_sample creates CalibrationSample from review resolution
- was_correct=True for approvals, was_correct=False for corrections
- Detects correction type (amount, name, match, multiple)
- Extracts document type from agent checkpoints
- Categorizes into confidence bucket (high/medium/low)
  </done>
</task>

<task type="auto">
  <name>Task 2: Update manual review resolution to capture calibration</name>
  <files>
    app/routers/manual_review.py
    app/services/validation/review_queue.py
  </files>
  <action>
Update the manual review resolution endpoint to capture calibration samples when items are resolved.

1. Read current `app/routers/manual_review.py` to understand existing resolve endpoint structure.

2. Update the resolve endpoint (`POST /manual-review/{review_id}/resolve`) to:
   - Accept optional `corrected_data` in request body for corrections
   - After successful resolution, call capture_calibration_sample
   - Handle both "approved" (no changes) and "corrected" (with changes) resolutions

3. Update the resolution request schema if needed:
```python
class ResolveReviewRequest(BaseModel):
    resolution: str  # approved, corrected, rejected, spam, escalated
    notes: Optional[str] = None
    corrected_data: Optional[Dict[str, Any]] = None  # For corrections
```

4. In the resolve endpoint handler:
```python
from app.services.calibration import capture_calibration_sample

# After successful resolution update...
if resolution in ("approved", "corrected"):
    # Load email for calibration
    email = db.query(IncomingEmail).filter(IncomingEmail.id == review_item.email_id).first()
    if email:
        capture_calibration_sample(
            db=db,
            review_item=review_item,
            email=email,
            resolution=request.resolution,
            corrected_data=request.corrected_data
        )
```

5. Ensure the db.commit() happens after both resolution and calibration capture (atomic).

Note: Do NOT modify the core resolution logic - only ADD the calibration capture as a side effect.
  </action>
  <verify>
python -c "from app.routers.manual_review import router; print([r.path for r in router.routes])"
  </verify>
  <done>
- Resolve endpoint accepts optional corrected_data for corrections
- Calibration sample captured for approved and corrected resolutions
- Spam/rejected/escalated resolutions skip calibration capture
- All changes atomic within same transaction
  </done>
</task>

</tasks>

<verification>
After both tasks complete:
1. `python -c "from app.services.calibration import capture_calibration_sample; print('service ok')"`
2. Verify router has resolve endpoint: `python -c "from app.routers.manual_review import router; paths = [r.path for r in router.routes]; print('/manual-review/{review_id}/resolve' in str(paths) or 'resolve' in str(paths))"`
3. Grep for calibration import in router: `grep -l "capture_calibration_sample" app/routers/manual_review.py`
</verification>

<success_criteria>
- Calibration collector creates samples with correct labels based on resolution
- "approved" resolution = was_correct=True
- "corrected" resolution = was_correct=False with correction details
- Correction type detected (amount_corrected, name_corrected, match_corrected, multiple)
- Document type extracted from agent checkpoints
- Manual review resolve endpoint triggers calibration capture
- Spam/rejected/escalated resolutions skipped (not useful for calibration)
</success_criteria>

<output>
After completion, create `.planning/phases/07-confidence-scoring-calibration/07-03-SUMMARY.md`
</output>
